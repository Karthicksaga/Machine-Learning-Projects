{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "h43kDT3M41u5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X_train:(3000, 5) and Y_train:(3000,)\n",
      "Shape of the X_test : (1000, 5) and y_test : (1000,)\n",
      "Shape of the Validation :(1000, 5) and Y_cv : (1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=0.001)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\n",
    "X_train_final,X_cv,y_train_final,y_cv = train_test_split(X_train,y_train,test_size = 0.25,random_state = 0)\n",
    "#train the algorithm\n",
    "print(\"Shape of the X_train:{0} and Y_train:{1}\".format(X_train_final.shape,y_train_final.shape))\n",
    "print(\"Shape of the X_test : {0} and y_test : {1}\".format(X_test.shape,y_test.shape))\n",
    "print(\"Shape of the Validation :{0} and Y_cv : {1}\".format(X_cv.shape,y_cv.shape))\n",
    "support_vector_classifier = SVC(gamma = 0.001,C = 100)\n",
    "support_vector_classifier.fit(X_train_final,y_train_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorClassifier:\n",
    "    \n",
    "    def __init__(self,gamma = 0.001,C = 100):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        \n",
    "        model = SVC(gamma = self.gamma,C = self.C).fit(X_train,y_train)\n",
    "        return model\n",
    "    \n",
    "    def compute_similarity_matrix(self,xi,xq,gamma):\n",
    "    \n",
    "        norm = np.sum(( xi - xq) ** 2 ,axis = -1)\n",
    "\n",
    "        similarity_function  = np.exp(- (gamma) * norm)\n",
    "\n",
    "        return similarity_function\n",
    "    \n",
    "    def kernal_gram_matrix(self,Xi,Xj,gamma):\n",
    "        kernal_matrix_similarity = np.zeros((Xi.shape[0],Xj.shape[0])) \n",
    "        for idx,row in enumerate(Xi):\n",
    "            for idx_col,column in enumerate(Xj):\n",
    "                rbf_kernal = self.compute_similarity_matrix(row,column,self.gamma)\n",
    "                kernal_matrix_similarity[idx][idx_col] = rbf_kernal\n",
    "        return kernal_matrix_similarity\n",
    "        \n",
    "    def decision_function(self,XCV,support_vectors,dual_coeff,intercept):\n",
    "    \n",
    "        #compute the kernal similarity Matrix\n",
    "        kernal_gram_matixes= self.kernal_gram_matrix(XCV, support_vectors,self.gamma)\n",
    "\n",
    "        #yi* alpha i * kernal(xi,xq)\n",
    "        decision_fun = dual_coeff * kernal_gram_matixes\n",
    "        decision_custom = np.sum(decision_fun,axis = -1) + intercept\n",
    "\n",
    "        return decision_custom.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SupportVectorClassifier(gamma = 0.001,C = 100)\n",
    "support_vector_classifier = svc.fit(X_train_final,y_train_final)\n",
    "#intercept b\n",
    "intercept = support_vector_classifier.intercept_\n",
    "\n",
    "#dual_coefficient yi*alphai\n",
    "\n",
    "svm_dual_coeff_ = support_vector_classifier.dual_coef_\n",
    "\n",
    "#get the support_vector\n",
    "support_vector = support_vector_classifier.support_vectors_\n",
    "\n",
    "f_cv = svc.decision_function(X_cv,support_vector,svm_dual_coeff_,intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_df = support_vector_classifier.decision_function(X_cv)\n",
    "sk_df = sk_df.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#check the difference between the custom predction and actual predictiom\n",
    "difference = f_cv - sk_df\n",
    "differece_error = [i  for i in range(len(difference)) if difference[i] > 1e-10]\n",
    "print(differece_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Label : 906 and Negative Label : 2094\n",
      "Calibrated positive point:0.998898678414097 and Calibrated Negative Point : 0.00047709923664122136\n"
     ]
    }
   ],
   "source": [
    "#implemented Calibrated CV\n",
    "\n",
    "#input actual_y -> function(x)-> ycv\n",
    "#construct a data set\n",
    "\n",
    "def label_count(Y):\n",
    "    positive_label,negative_label = 0,0\n",
    "    for label in range(len(Y)):\n",
    "        if Y[label] == 1:\n",
    "            positive_label += 1\n",
    "        else:\n",
    "            negative_label +=1\n",
    "    assert((positive_label +negative_label) == len(Y) )\n",
    "    return positive_label,negative_label\n",
    "no_of_positive_point,no_of_negative_point = label_count(y_train_final)\n",
    "print(\"Positive Label : {0} and Negative Label : {1}\".format(no_of_positive_point,no_of_negative_point))\n",
    "y_positive,y_negative = (no_of_positive_point + 1) / (no_of_positive_point + 2), 1 / (no_of_negative_point + 2)\n",
    "print(\"Calibrated positive point:{0} and Calibrated Negative Point : {1}\".format(y_positive,y_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In platt Scaling We build a new validation dataset- create a bucket and computing the averagefrom our case have no of points in low we directly apply y_avg\n",
    "def average_y(Y,y_positive,y_negative):\n",
    "    \n",
    "    y_bucket = []\n",
    "    for label in range(len(Y)):\n",
    "        if Y[label] == 1:\n",
    "            y_bucket.append(y_positive)\n",
    "        else:\n",
    "            y_bucket.append(y_negative)\n",
    "    return y_bucket\n",
    "y_cv_modified = average_y(y_cv,y_positive,y_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self,learning_rate,epochs):\n",
    "        \n",
    "        self.learning_rate = learning_rate \n",
    "        self.epochs = epochs \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        sigmoid_of_Z= 1 / (1 + np.exp(-z))\n",
    "        return sigmoid_of_Z\n",
    "    \n",
    "    def gradient(self,x,y,w,b,alpha,N):\n",
    "        dw = ((x * ((y - self.sigmoid(np.dot(w,x) + b))) - ((alpha / N) * w)))\n",
    "        db = y - self.sigmoid(np.dot(w,x) + b)\n",
    "        return dw,db\n",
    "    def logloss(self,y_true,y_pred):\n",
    "        \n",
    "        loss = 0\n",
    "        n = len(y_true)\n",
    "        if(len(y_true) == len(y_pred)):\n",
    "            for l in range(len(y_true)):\n",
    "\n",
    "                loss += (y_true[l] * math.log10(y_pred[l])) + ((1-y_true[l]) * math.log10(1-y_pred[l]))\n",
    "\n",
    "            loss = (-1 / n) * loss \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self,X,Y,alpha):\n",
    "        \n",
    "        loss_list = []\n",
    "        wait = 0\n",
    "        #initialize weight\n",
    "        w = 0\n",
    "        b = 0\n",
    "        print(self.learning_rate)\n",
    "        for epoch in range(1,epochs+1):\n",
    "            \n",
    "            for data in range(len(X)):      \n",
    "                #compute gradient\n",
    "                dw,db= self.gradient(X[data],Y[data],w,b,alpha,len(X))\n",
    "                #update weight \n",
    "                w = w +  self.learning_rate * dw\n",
    "                #update bias\n",
    "                b = b +  self.learning_rate * db\n",
    "            y_pred  = [self.sigmoid(np.dot(w,X[data]) + b) for data in range(len(X))]\n",
    "            loss = self.logloss(Y,np.array(y_pred))\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            if epoch >= 2:\n",
    "                if loss_list[epoch-2]  == loss_list[epoch-1]:\n",
    "                    print(\"No improvement in training\")\n",
    "                    break\n",
    "                     \n",
    "            print('Epoch--{0}'.format(epoch) )\n",
    "\n",
    "            print(\"Training Loss  : {0} \".format(loss[0]))\n",
    "\n",
    "        return loss_list,w,b\n",
    "    def predict(self,x,w,b):\n",
    "        z = np.dot(w,x) + b\n",
    "        return self.sigmoid(z)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "alpha=0.0001\n",
    "y_cv_modified = np.array(y_cv_modified).reshape(-1,1)\n",
    "sgd = SGD(learning_rate,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Epoch--1\n",
      "Training Loss  : 0.2623439039314991 \n",
      "Epoch--2\n",
      "Training Loss  : 0.23328949504531726 \n",
      "Epoch--3\n",
      "Training Loss  : 0.21112989001987448 \n",
      "Epoch--4\n",
      "Training Loss  : 0.19390095342750988 \n",
      "Epoch--5\n",
      "Training Loss  : 0.1802395179581706 \n",
      "Epoch--6\n",
      "Training Loss  : 0.16920443188047013 \n",
      "Epoch--7\n",
      "Training Loss  : 0.16014004963550305 \n",
      "Epoch--8\n",
      "Training Loss  : 0.15258251814728516 \n",
      "Epoch--9\n",
      "Training Loss  : 0.14619776722588454 \n",
      "Epoch--10\n",
      "Training Loss  : 0.14074077868139112 \n",
      "Epoch--11\n",
      "Training Loss  : 0.13602866186090992 \n",
      "Epoch--12\n",
      "Training Loss  : 0.1319226272846691 \n",
      "Epoch--13\n",
      "Training Loss  : 0.12831572881244926 \n",
      "Epoch--14\n",
      "Training Loss  : 0.12512439016558863 \n",
      "Epoch--15\n",
      "Training Loss  : 0.12228245064105836 \n",
      "Epoch--16\n",
      "Training Loss  : 0.11973691421734095 \n",
      "Epoch--17\n",
      "Training Loss  : 0.11744486869036001 \n",
      "Epoch--18\n",
      "Training Loss  : 0.11537122090892515 \n",
      "Epoch--19\n",
      "Training Loss  : 0.1134870096789125 \n",
      "Epoch--20\n",
      "Training Loss  : 0.11176813331551005 \n",
      "Epoch--21\n",
      "Training Loss  : 0.11019437877586269 \n",
      "Epoch--22\n",
      "Training Loss  : 0.10874867287090426 \n",
      "Epoch--23\n",
      "Training Loss  : 0.10741649892603101 \n",
      "Epoch--24\n",
      "Training Loss  : 0.1061854380530169 \n",
      "Epoch--25\n",
      "Training Loss  : 0.10504480524062751 \n",
      "Epoch--26\n",
      "Training Loss  : 0.10398535829020995 \n",
      "Epoch--27\n",
      "Training Loss  : 0.10299906322149323 \n",
      "Epoch--28\n",
      "Training Loss  : 0.10207890382703495 \n",
      "Epoch--29\n",
      "Training Loss  : 0.10121872601838272 \n",
      "Epoch--30\n",
      "Training Loss  : 0.10041310979663653 \n",
      "Epoch--31\n",
      "Training Loss  : 0.09965726331235482 \n",
      "Epoch--32\n",
      "Training Loss  : 0.09894693470715851 \n",
      "Epoch--33\n",
      "Training Loss  : 0.09827833836006895 \n",
      "Epoch--34\n",
      "Training Loss  : 0.0976480928728578 \n",
      "Epoch--35\n",
      "Training Loss  : 0.09705316867628976 \n",
      "Epoch--36\n",
      "Training Loss  : 0.09649084356375484 \n",
      "Epoch--37\n",
      "Training Loss  : 0.09595866479027237 \n",
      "Epoch--38\n",
      "Training Loss  : 0.09545441663524254 \n",
      "Epoch--39\n",
      "Training Loss  : 0.09497609253320682 \n",
      "Epoch--40\n",
      "Training Loss  : 0.09452187104051173 \n",
      "Epoch--41\n",
      "Training Loss  : 0.09409009503663503 \n",
      "Epoch--42\n",
      "Training Loss  : 0.0936792536640946 \n",
      "Epoch--43\n",
      "Training Loss  : 0.09328796659580965 \n",
      "Epoch--44\n",
      "Training Loss  : 0.09291497028774075 \n",
      "Epoch--45\n",
      "Training Loss  : 0.09255910593087173 \n",
      "Epoch--46\n",
      "Training Loss  : 0.09221930886265292 \n",
      "Epoch--47\n",
      "Training Loss  : 0.091894599235923 \n",
      "Epoch--48\n",
      "Training Loss  : 0.09158407377461211 \n",
      "Epoch--49\n",
      "Training Loss  : 0.09128689847149252 \n",
      "Epoch--50\n",
      "Training Loss  : 0.09100230210482986 \n",
      "Epoch--51\n",
      "Training Loss  : 0.09072957046884425 \n",
      "Epoch--52\n",
      "Training Loss  : 0.09046804122800774 \n",
      "Epoch--53\n",
      "Training Loss  : 0.09021709931792915 \n",
      "Epoch--54\n",
      "Training Loss  : 0.08997617282631465 \n",
      "Epoch--55\n",
      "Training Loss  : 0.08974472929657472 \n",
      "Epoch--56\n",
      "Training Loss  : 0.08952227240436805 \n",
      "Epoch--57\n",
      "Training Loss  : 0.08930833896393585 \n",
      "Epoch--58\n",
      "Training Loss  : 0.08910249622669368 \n",
      "Epoch--59\n",
      "Training Loss  : 0.08890433943934649 \n",
      "Epoch--60\n",
      "Training Loss  : 0.08871348963291924 \n",
      "Epoch--61\n",
      "Training Loss  : 0.08852959161763156 \n",
      "Epoch--62\n",
      "Training Loss  : 0.08835231216162523 \n",
      "Epoch--63\n",
      "Training Loss  : 0.08818133833417724 \n",
      "Epoch--64\n",
      "Training Loss  : 0.08801637599634181 \n",
      "Epoch--65\n",
      "Training Loss  : 0.0878571484239491 \n",
      "Epoch--66\n",
      "Training Loss  : 0.08770339504962435 \n",
      "Epoch--67\n",
      "Training Loss  : 0.0875548703120069 \n",
      "Epoch--68\n",
      "Training Loss  : 0.08741134260166905 \n",
      "Epoch--69\n",
      "Training Loss  : 0.08727259329439284 \n",
      "Epoch--70\n",
      "Training Loss  : 0.0871384158634822 \n",
      "Epoch--71\n",
      "Training Loss  : 0.0870086150636789 \n",
      "Epoch--72\n",
      "Training Loss  : 0.08688300618004377 \n",
      "Epoch--73\n",
      "Training Loss  : 0.08676141433584818 \n",
      "Epoch--74\n",
      "Training Loss  : 0.08664367385414927 \n",
      "Epoch--75\n",
      "Training Loss  : 0.08652962766825541 \n",
      "Epoch--76\n",
      "Training Loss  : 0.08641912677677396 \n",
      "Epoch--77\n",
      "Training Loss  : 0.086312029739364 \n",
      "Epoch--78\n",
      "Training Loss  : 0.08620820220969132 \n",
      "Epoch--79\n",
      "Training Loss  : 0.08610751650243058 \n",
      "Epoch--80\n",
      "Training Loss  : 0.08600985119145745 \n",
      "Epoch--81\n",
      "Training Loss  : 0.0859150907366405 \n",
      "Epoch--82\n",
      "Training Loss  : 0.08582312513689555 \n",
      "Epoch--83\n",
      "Training Loss  : 0.08573384960736886 \n",
      "Epoch--84\n",
      "Training Loss  : 0.08564716427882373 \n",
      "Epoch--85\n",
      "Training Loss  : 0.08556297391746323 \n",
      "Epoch--86\n",
      "Training Loss  : 0.08548118766360062 \n",
      "Epoch--87\n",
      "Training Loss  : 0.08540171878770683 \n",
      "Epoch--88\n",
      "Training Loss  : 0.08532448446251112 \n",
      "Epoch--89\n",
      "Training Loss  : 0.08524940554993635 \n",
      "Epoch--90\n",
      "Training Loss  : 0.08517640640175544 \n",
      "Epoch--91\n",
      "Training Loss  : 0.08510541467295472 \n",
      "Epoch--92\n",
      "Training Loss  : 0.08503636114686645 \n",
      "Epoch--93\n",
      "Training Loss  : 0.08496917957122135 \n",
      "Epoch--94\n",
      "Training Loss  : 0.0849038065043314 \n",
      "Epoch--95\n",
      "Training Loss  : 0.08484018117068305 \n",
      "Epoch--96\n",
      "Training Loss  : 0.08477824532527921 \n",
      "Epoch--97\n",
      "Training Loss  : 0.08471794312611652 \n",
      "Epoch--98\n",
      "Training Loss  : 0.0846592210142411 \n",
      "Epoch--99\n",
      "Training Loss  : 0.0846020276008571 \n",
      "Epoch--100\n",
      "Training Loss  : 0.08454631356101536 \n",
      "Optimized W:[1.44961872] and b:[-0.18626539]\n"
     ]
    }
   ],
   "source": [
    "loss_list,w,b= sgd.fit(f_cv,y_cv_modified,alpha)\n",
    "print(\"Optimized W:{0} and b:{1}\".format(w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(range(1,epochs + 1),loss_list)\n",
    "plt.scatter(range(1,epochs+1),loss_list)\n",
    "plt.title(\"Log Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test Data \n",
    "ftest = svc.decision_function(X_test,support_vector,svm_dual_coeff_,intercept)\n",
    "test_prediction  = [sgd.predict(ftest[data],w[0],b[0])[0] for data in range(len(ftest))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Prediction : [0.004969085753400267, 0.47172988567687335, 0.06292162797181014, 0.4095663384280869, 0.01140033712095906, 0.9259763715110881, 0.12365638516541687, 0.046788111997427245, 0.0091663688641821, 0.587843417715812, 0.015006597986503008, 0.00396070783736178, 0.029818039457043463, 0.14609579168440315, 0.014945209957721714, 0.03131214377426784, 0.9345603976529385, 0.0052418777140559245, 0.4110100860659789, 0.8895417931530457, 0.014402329226791405, 0.002199656329178635, 0.009941930326802193, 0.13125844227504047, 0.9466558956954717, 0.028231504596045367, 0.965887174631899, 0.7253491644589101, 0.948790049442062, 0.0019381617031063703, 0.01344490821904161, 0.08800046071647795, 0.02966517906702783, 0.9688470781895671, 0.038380802429151935, 0.018434573396832455, 0.022748086979284786, 0.806287544725349, 0.892218807517814, 0.020030795953398742, 0.8998408320921583, 0.03615966297659763, 0.17937112970496885, 0.45612959165443817, 0.02573608492544919, 0.008026124639616194, 0.958247539026555, 0.8539575223687195, 0.024180882862586836, 0.8968458788928236, 0.030012366497741493, 0.7835538366509989, 0.9553303684625589, 0.014202752891805257, 0.0539202064150608, 0.6597687572978111, 0.0054050745074294745, 0.8608393647651055, 0.0022143795629986553, 0.025794088814771014, 0.8902287186289125, 0.9504415022696011, 0.2488237642383243, 0.39010548232849696, 0.8639629160260216, 0.7302045241577895, 0.2544252845808563, 0.04714246543115774, 0.8142335830158376, 0.15777663425233646, 0.07132266205980414, 0.005880824807556567, 0.007625974715235556, 0.8452587836266152, 0.8456769108952207, 0.9296716072186454, 0.3499841377882236, 0.8072556703568875, 0.9498612381451831, 0.2658891507486397, 0.08364924976124825, 0.0033073348738980463, 0.010509823039617118, 0.8584156005944402, 0.011041964521947594, 0.41141101960332166, 0.3150029323290646, 0.9166495157867433, 0.030519894810188714, 0.0047820049979251066, 0.057633478052011876, 0.18174992034862006, 0.8855282672161411, 0.8808485636288477, 0.9960134812790495, 0.006370944944524001, 0.010043439410350064, 0.032557951593329276, 0.5331007155772204, 0.019868402083566796, 0.9290117624814149, 0.034290125948586714, 0.04055156531874831, 0.0979613270570139, 0.37084943765334166, 0.005263772996877466, 0.06538537805672935, 0.00624614578572894, 0.00023554922034736125, 0.5171147407787401, 0.8324093313802556, 0.018927157105622403, 0.05920888642780235, 0.0015135669898467736, 0.3178972546911113, 0.9401033885550046, 0.9181950637936352, 0.24099072727277512, 0.012588693851300084, 0.8008398223108214, 0.7155478403936302, 0.035677973439124724, 0.04166456408577889, 0.04850990530708536, 0.005925588712795146, 0.067546593696552, 0.0043779180921433205, 0.006696283822948996, 0.8823764432122138, 0.21573323705842623, 0.04121885356200099, 0.8373895031423978, 0.003456417077944913, 0.8641679588178953, 0.003831579611661379, 0.0065301085397466985, 0.00946732460767537, 0.041402210539976586, 0.022411622351850853, 0.5366319044042956, 0.008691779496129145, 0.6287954774357731, 0.09373895900730056, 0.03809431105766375, 0.9801852226553932, 0.9375013013367913, 0.08458413183104609, 0.7823134835907389, 0.019399350267034928, 0.03397792983669366, 0.01307187064626153, 0.004163884438875327, 0.012139062968719519, 0.7373334282401842, 0.008535996242115715, 0.3665590787115734, 0.014096605578407587, 0.025381306064780847, 0.6439357444238846, 0.03693760816724338, 0.09617083556940423, 0.006823839075358012, 0.2082559925455328, 0.025277238901981115, 0.5653988779966552, 0.9205868984762204, 0.02094339129613084, 0.05413481494309434, 0.9152077235984286, 0.8806850094678386, 0.5109881603239929, 0.023159201711443447, 0.6979043938723505, 0.0016307473632282226, 0.017154186811187282, 0.03557663026602945, 0.023946902596287057, 0.0022722657915608766, 0.8963698100590378, 0.028467493390487677, 0.7800706031231566, 0.8497327839350607, 0.021116382453205487, 0.16995354366558602, 0.02333454474884013, 0.0152684230521062, 0.016915846615419766, 0.7554714178279204, 0.13330549339244072, 0.0036350637598909384, 0.038272692301725024, 0.8278841303166204, 0.8211667217211371, 0.0060130160655642185, 0.15026217629933938, 0.013572066201862285, 0.0148799086246874, 0.16500089015362723, 0.009673834515673479, 0.0011592600233922337, 0.0038807482310747547, 0.000965074838795534, 0.5990877444101601, 0.9932564676312478, 0.013985845812317958, 0.9127276885519151, 0.1319287181529915, 0.14971036293571285, 0.5180186777706162, 0.9066598188025005, 0.06969656732210415, 0.6513611359419914, 0.03881487905647955, 0.001825190302476183, 0.8891102151104239, 0.9153939760166888, 0.012506345492306748, 0.5637171178935163, 0.04073224380053033, 0.7838279507937469, 0.9639621915152675, 0.9106857354874239, 0.10460068723975229, 0.042192952677736505, 0.912246240603363, 0.8869816725114472, 0.10038113558353977, 0.8810818133621212, 0.00415860873388059, 0.3054677544133251, 0.19122719111715467, 0.038136983749512174, 0.014674154072689887, 0.3134846837836528, 0.05893551644832439, 0.8460992065482258, 0.021744654392935082, 0.2767222846511429, 0.9152773859375932, 0.002416601755752803, 0.0027542798351156837, 0.04643254623173652, 0.006366716891215406, 0.9590998127786221, 0.021245478414767528, 0.20583503368657008, 0.7319861560756777, 0.03182603819939563, 0.035826047144150004, 0.026886755585613074, 0.05019931915975108, 0.011401659878814543, 0.1600804112938701, 0.0019660202022518014, 0.09153841670702809, 0.8989851192847265, 0.7513455254732313, 0.07907632373097792, 0.981095277496359, 0.7916562878750087, 0.9251754259419104, 0.20597658250289974, 0.9676934316737842, 0.003754628332879084, 0.5298728968763671, 0.036746773753917744, 0.015347781364898345, 0.8871424032083762, 0.019360749810780224, 0.10880444455595958, 0.010778960275007314, 0.9386057324603535, 0.06003416752769395, 0.13272415024276268, 0.022078768216568824, 0.9442177047396915, 0.21227192535551162, 0.009826128477741667, 0.0132330514499037, 0.8876952448731813, 0.007957737004217298, 0.8260651596144029, 0.4759844352483603, 0.005493828555075233, 0.20344716266041377, 0.31585195965507534, 0.9871403701683222, 0.9067551266543815, 0.011144008656955265, 0.17670241546978677, 0.007637504356212188, 0.9484613862203669, 0.03763541866824508, 0.10389149303766163, 0.14343254930391894, 0.8821258113230699, 0.0230080983776724, 0.022458199579819104, 0.6452778883857878, 0.2269955543381045, 0.06038783179359782, 0.03973977950674534, 0.009981121455773286, 0.5317631884913141, 0.023135437569218115, 0.0057520287285149135, 0.004448346823424566, 0.005684123549974109, 0.9334539593091518, 0.12445541459445243, 0.034851719377517784, 0.11878512517990605, 0.8890974398936752, 0.6613871187527062, 0.002153561718730909, 0.02484156511710386, 0.006433222801387303, 0.07524221522734724, 0.8554451862726993, 0.9065341777922162, 0.14760399751175507, 0.8622749722801535, 0.011310636132740196, 0.8948170447600673, 0.9622146410889729, 0.5193516010215815, 0.17428101213977082, 0.8439194767357363, 0.8459059600738495, 0.008382478191375872, 0.020078929268940036, 0.24599213020816207, 0.0030244162334840453, 0.013263046492265094, 0.014042190564353987, 0.9518891183208489, 0.012120061586774322, 0.07411487304537728, 0.9029268028549483, 0.013203763078525594, 0.016897584629914315, 0.9683245235928053, 0.0007975520479262399, 0.021967835039204544, 0.01756316362703198, 0.9239448315175238, 0.042274868618081336, 0.049846317025661425, 0.010002191236835893, 0.8416134186215078, 0.864423775656, 0.0026532549650931845, 0.05102197698433873, 0.003131039922159221, 0.850313041472486, 0.8716846824165186, 0.4412786086822296, 0.10351699430370714, 0.9196232183587024, 0.0035334217722255545, 0.03618321147642307, 0.5806427663912433, 0.012980823460820908, 0.48966021674155225, 0.5983973980520698, 0.11600097494268943, 0.1653631157173856, 0.001327635845819174, 0.9760170186735415, 0.15613789431108666, 0.9532657878948447, 0.012755032161757753, 0.004444890861787377, 0.012088352350468647, 0.9143051919914392, 0.04139429863145552, 0.002765828354813565, 0.8992980934018229, 0.01031393975987417, 0.9895901482203027, 0.03556765696384337, 0.9071146373132947, 0.9591846355909776, 0.0038900575676323675, 0.8452267026530209, 0.9018928827902665, 0.032869141048295435, 0.0571992384523265, 0.18507598024031977, 0.009206086843662158, 0.20160350391118922, 0.06699131245382793, 0.04019860261823433, 0.021561988463000933, 0.049038475664102445, 0.008017087795046694, 0.01950442754793142, 0.012129213461018041, 0.06647133008464381, 0.01766441544216768, 0.6334523323878094, 0.08071451935947646, 0.023438944764285517, 0.03159225999300921, 0.05174920708364413, 0.9473378559812201, 0.815290671832673, 0.006591476942438054, 0.0009657819306343415, 0.13298301296470835, 0.8256118760367751, 0.40811987510150066, 0.027782308058782134, 0.013724556280730544, 0.0341393163373114, 0.02200096382620749, 0.03999942036736662, 0.22361499674939364, 0.30622220332844435, 0.0035523044769656476, 0.9009759565142053, 0.9421585221985304, 0.0029663448505672634, 0.883632505228418, 0.49088348274834104, 0.9004728112572871, 0.4563621274998662, 0.6649914590466938, 0.0049232764868577545, 0.1051007014958135, 0.006289540746846559, 0.23137679873779693, 0.10635793591525722, 0.9033650068167899, 0.007430106825292272, 0.04502282560153768, 0.005919406176566327, 0.21662905359838133, 0.9748427812821264, 0.024980307018034047, 0.012695896845469111, 0.38790398043983765, 0.12517934586510243, 0.06417250342815611, 0.10416044806242727, 0.013339458384492811, 0.3164543792401975, 0.05654619948917097, 0.02718233265970043, 0.010892943653481526, 0.01232742000146449, 0.030169442687219977, 0.8849448892431654, 0.05099470782307198, 0.16098733052177758, 0.07874055175932534, 0.011504080530474442, 0.9466758030796915, 0.019455515056851436, 0.013498867002000165, 0.008643669602428142, 0.02338471173817452, 0.013319960516538745, 0.9280546571835809, 0.005358179151434211, 0.0021746540720897936, 0.8872282396662406, 0.03042378759553967, 0.9499197571394152, 0.016187809653018952, 0.9175728488476436, 0.8971484393203376, 0.9210042203193037, 0.02028372847601688, 0.01975449872999804, 0.10404414569873884, 0.014908736338101605, 0.8544657583869762, 0.03562298139977468, 0.04360820906189882, 0.7093189524710917, 0.013786930062980268, 0.1867957336381786, 0.006929643611400418, 0.3802110567790626, 0.507948783701398, 0.04428398609241807, 0.9573522552259038, 0.6708093726168366, 0.015250607835016852, 0.30871986811200813, 0.05650616936854968, 0.674411271338204, 0.7796600510242325, 0.009024976942831622, 0.00913929455070158, 0.031520081118525674, 0.0392908461590666, 0.03878263525277148, 0.012835422469578861, 0.8599488000758413, 0.9504408373839542, 0.007085082093750249, 0.002619772491497284, 0.013351528883867583, 0.0038944592671321873, 0.012634325926598843, 0.014704950917206408, 0.037079434654996093, 0.14035674313496754, 0.01790349438768708, 0.006540237611462195, 0.001421276081724003, 0.8796245044557817, 0.021575417579724483, 0.0912404492606723, 0.003812637609558308, 0.019647117670580392, 0.05971493605031084, 0.024742924735497978, 0.030848724225876343, 0.011438104635767875, 0.03085705375939177, 0.7838758698436921, 0.288799813597528, 0.9836701316106832, 0.8813830413583778, 0.8737880704411254, 0.03868799735631634, 0.011796265392701177, 0.16767266030366795, 0.8431464028203274, 0.023671285440176888, 0.8963253216082507, 0.025679078772543808, 0.9299345518993907, 0.0819624509581768, 0.025298920597021848, 0.5943828031902237, 0.03522422263113214, 0.013726617427364822, 0.003425073615616315, 0.35117174925963834, 0.05158145707433851, 0.8130221478548042, 0.05060690915214314, 0.9615179020912212, 0.09920498279554311, 0.022676639893045254, 0.2554300685757442, 0.006817175923268844, 0.08591775701317345, 0.0026809869685376925, 0.9856877278007821, 0.1783741451159112, 0.006680501438531301, 0.0037118513646032403, 0.00816687861083103, 0.14745797306362615, 0.29741923544897997, 0.004064232034985723, 0.9359169589042367, 0.006091212560470382, 0.016710196107522897, 0.025478674368288403, 0.020048409920860256, 0.9642024677435483, 0.08419739977307612, 0.014127874479255599, 0.6009519513196845, 0.01416028203410078, 0.004581904326785543, 0.09244052071536543, 0.008933881548571724, 0.011007721461224685, 0.4613970286657202, 0.6901600720965295, 0.0031193401638770486, 0.020660253239829528, 0.005771860729190929, 0.9269014670290131, 0.005086111483081345, 0.9695798811835467, 0.16145665742492143, 0.026885399743094156, 0.012525738195562254, 0.9146122028506933, 0.010530643706610882, 0.6084446903121608, 0.02387938029232971, 0.31740082293097716, 0.0391085135396463, 0.26661880153708695, 0.17425509772695602, 0.5486176388159295, 0.05336729987117464, 0.2539698656856387, 0.007030603547993043, 0.020052611583447462, 0.06650590890298948, 0.022811662218114204, 0.021482974752282717, 0.017621106275502025, 0.015350157595153478, 0.03296992384373664, 0.008972533397667244, 0.9230413903771499, 0.1706559260900858, 0.9277899845556469, 0.007219408124422209, 0.7316874534858469, 0.16668905275020796, 0.01208412169002136, 0.03570852349550301, 0.9049256462338913, 0.017385098032625577, 0.0834229060689596, 0.010886175177360301, 0.9398837542766053, 0.022734684024799397, 0.17842440421891617, 0.5507601504354224, 0.07769274617935601, 0.0047019916132491416, 0.05748617486891963, 0.002560733409678765, 0.09833851804314793, 0.9158921733509944, 0.6732704113203809, 0.8469927194411899, 0.010906140975146368, 0.7711380524411919, 0.1719186563902077, 0.014597140651016981, 0.9179392140254585, 0.05479196324803622, 0.010708973534896154, 0.00617975488176183, 0.04859755002280579, 0.2771923319999139, 0.0021365423676156414, 0.037243562185695495, 0.0335624413734091, 0.0038476466633211963, 0.006530485159845803, 0.011259160544640873, 0.21517370767695482, 0.053168131926924544, 0.8861796749244987, 0.029321836739419092, 0.0022745912677754475, 0.28830994307037755, 0.012769475315012633, 0.8585358719075178, 0.06762661784621822, 0.9357945877186702, 0.9155072612844136, 0.29810960395890934, 0.08305086021353059, 0.7927617956593797, 0.016672024942644626, 0.023792887288902935, 0.8990633662028513, 0.9692372349822839, 0.025010287196864888, 0.9166863038274449, 0.05815154193610908, 0.009018983245797007, 0.7023122882474042, 0.0056859607415298465, 0.043592097798880014, 0.2663285270148487, 0.05810440753248776, 0.130101638020477, 0.03511727117097223, 0.07620309118819343, 0.18714282542714866, 0.9194360159114736, 0.03359403649757745, 0.8511469688312853, 0.022856672881365222, 0.750834235553276, 0.015382652336346642, 0.007165546074266068, 0.036930214742453636, 0.0026838805160135817, 0.0038359006302866442, 0.915356666841629, 0.218150172100777, 0.9049433299100443, 0.011750920123961695, 0.010023893262591023, 0.8934257140182094, 0.8029618278603653, 0.747008421060196, 0.008593764979204284, 0.02996907921746544, 0.6155475337375084, 0.0211763212254634, 0.009912740467532235, 0.018439362842612113, 0.017305169232395442, 0.9416052796939323, 0.1380082655917911, 0.001135950516825506, 0.0017136903110065087, 0.014486499688404614, 0.059424139241715586, 0.8448779429936464, 0.8892990916898387, 0.006144656387913832, 0.017550240315264573, 0.009326198918709344, 0.029899133580421726, 0.02686076338921893, 0.908417515526719, 0.6400207083563656, 0.7876153433511827, 0.8974987644250132, 0.9147832886549552, 0.0015220782917049998, 0.008119529452721543, 0.331211301989774, 0.7488180716581544, 0.9265947367580925, 0.7267677588254423, 0.9419262202713251, 0.05981088461270495, 0.23295671961178513, 0.008886082162941669, 0.012342076450067095, 0.020892375838459782, 0.935858635614539, 0.6579054254683824, 0.012396186542942172, 0.02081880310414923, 0.017801901127801948, 0.1878066744284011, 0.010177641076990616, 0.03890445720080009, 0.01765232728499047, 0.06502926706122208, 0.8765215092402678, 0.7397615608370037, 0.27486109605606335, 0.9479916955738351, 0.02465342565787844, 0.8326859736589576, 0.7182272855530825, 0.08382999603575807, 0.03239958073538869, 0.7920483261970865, 0.015507175951933736, 0.03731840018645909, 0.0048674660861710625, 0.8708423819500741, 0.39499504286763615, 0.06401099276450813, 0.5949630776308059, 0.004586335828173447, 0.0020985934856826687, 0.8895962287797359, 0.9417913173302764, 0.1605388898605259, 0.7887246643407232, 0.012308527131255012, 0.36009235228617786, 0.9290619271464052, 0.03686087112240463, 0.09781701470486533, 0.010370883483724181, 0.012372824162919294, 0.008499231181105899, 0.9296902269361884, 0.002352179054386855, 0.03369684659425547, 0.15996007729890913, 0.7174928956269603, 0.10198160804423961, 0.15105760466582585, 0.9243855253336528, 0.05244429944016827, 0.009631449389443126, 0.0010247674931590648, 0.012180549695337682, 0.004254121318351341, 0.05169412886062728, 0.9025840647018418, 0.015292623047793746, 0.007881661448321134, 0.05240888344598458, 0.005533226261239993, 0.020240145633752954, 0.02190723643038961, 0.00807105161661193, 0.0031513751239677606, 0.6278077919469351, 0.0024500158939048267, 0.03401920687864047, 0.14249622772201398, 0.9138636847109306, 0.03638388841973371, 0.035625890473509246, 0.018992197687039497, 0.6765867889284505, 0.972435808923492, 0.014870964002812733, 0.05897775348723221, 0.03367497843227707, 0.014532753904144283, 0.693771219732998, 0.012736123255336124, 0.01009224163565739, 0.3035347837302824, 0.05578103359547396, 0.8954802968402146, 0.910114260422436, 0.0694316841148188, 0.060503339587433674, 0.9426315419850522, 0.6341560374389287, 0.0038182005299674, 0.013924738876956846, 0.6867944702993711, 0.8164955755077037, 0.015647430124640396, 0.015977454902225567, 0.22176045138882952, 0.009427555776346734, 0.004210367974297324, 0.22227465585690614, 0.03549594089390186, 0.9837665080048047, 0.9140866465206904, 0.12512095233978346, 0.017153943913095555, 0.8876968617244396, 0.0014278756671650837, 0.04338894251039712, 0.8113457986503221, 0.948326335831227, 0.09031417640910575, 0.9287223732067597, 0.010451443911545273, 0.7849842561487705, 0.9200655467385225, 0.0014121331287695353, 0.03203763123237309, 0.008126305290644613, 0.917070461739644, 0.7177726670716597, 0.05621505473951251, 0.9710198252120391, 0.020972439953739683, 0.0011301808995184545, 0.0766409345246784, 0.04443282902876281, 0.019746747861353, 0.9177768550977199, 0.8716673130464043, 0.818357112447188, 0.020868074579247112, 0.036125679008680196, 0.7868447637057217, 0.960943622792097, 0.23817467992663066, 0.9298253260959451, 0.0253083877486775, 0.5831317126668926, 0.006853071416702282, 0.0388177059385466, 0.9041420669999193, 0.007908025004451653, 0.10310326213799531, 0.0307817708064836, 0.04377526665088593, 0.005943186887325765, 0.35187812635262256, 0.012365600463797675, 0.04470435221465508, 0.05375496155892317, 0.005483950176306995, 0.01685209417995656, 0.5301854499695864, 0.9196219791673442, 0.016475663537916786, 0.010648231371324146, 0.8302216767295716, 0.006357235206929645, 0.035486633383354546, 0.5570264515066163, 0.4340720495071271, 0.006906605817688499, 0.012857362206669167, 0.029240981561905167, 0.934676538123794, 0.3331579367580997, 0.24760123285058952, 0.029528506495365918, 0.004797641580616669, 0.0027215327293875755, 0.005714108363574435, 0.00830560003625694, 0.013100271192047068, 0.9669014607094286, 0.9196246014250599, 0.005397675295964074, 0.007267347284055392, 0.7083580770204444, 0.928520110774463, 0.6605455077287288, 0.8418044139951117, 0.006161550128980891, 0.00485785092685666, 0.025163857010942897, 0.39312376612330774, 0.20682434178872866, 0.007013593450977939, 0.017321707140522898, 0.9259300162002735, 0.012806002828201657, 0.0538891350620549, 0.028582614308831696, 0.5356806814285673, 0.022094083833176632, 0.02729019242150993, 0.9025115434158038, 0.09779135862893151, 0.6972968658763775, 0.8604371371158968, 0.9141138279260204, 0.4942707446901395, 0.014470775990579167, 0.11712324204337933, 0.9274402542578638, 0.04991738712459486, 0.002917098550891854, 0.044498918103400086, 0.9325735277368814, 0.016266124978093374, 0.08257386753154072, 0.008171623671903232, 0.03141899759129818, 0.0799377316948801, 0.017593747771009145, 0.03032858188706741, 0.2048124102188875, 0.7083434199189101, 0.002351787740175269, 0.17287885215496823, 0.03974443891231009, 0.9932047416692786, 0.5257483612462376, 0.9873965917078832, 0.8513580935393782, 0.17350346993042481, 0.6291549626714529, 0.03047927616291394, 0.8972729995884771, 0.15290571951103898, 0.04169936808547081, 0.8963953841027106, 0.015041642465423038, 0.8248730331924743, 0.21594127766676025, 0.21609580409652762, 0.8201854772354967, 0.8661741157159882, 0.8989912314716288, 0.005434565152639596, 0.021670930117685536, 0.032604353355719186, 0.2185048491379299, 0.04208863320040992, 0.8580943734055915, 0.014807543265967128, 0.004003860434128941, 0.011943458043127908, 0.943578915360485, 0.31818394672342853, 0.055986533757678265, 0.016981112310441058, 0.014117699756059539, 0.2898402477956884, 0.8584909258045554, 0.22724881144595824, 0.007255466942573851, 0.07615545479677402, 0.002877487422903117, 0.7575088931260698, 0.9684576104696321, 0.05887364018633745, 0.02264634993884578, 0.021348349993282997, 0.01950367440858248, 0.018110053179802334, 0.02383904761334543, 0.4185637380723513, 0.01851174763730869, 0.011001208928550523]\n"
     ]
    }
   ],
   "source": [
    "print(\"test Prediction : {0}\".format(test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
