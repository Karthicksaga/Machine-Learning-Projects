{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc1PkAfrBMXd"
      },
      "source": [
        "#filepath=\"Best_weight-{epoch:02d}-{val_auroc:.2f}.hdf5\"\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kCMOTwELDfZX",
        "outputId": "84d5228d-fab2-4635-d6ac-01275ba987d2"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EazQYRrMDj2z",
        "outputId": "6ef2e1ab-bea2-4830-9c85-af151e80cbaa"
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 30\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmI9KIVgD7GM",
        "outputId": "917f6b48-d34f-4d26-b88e-b08a788cff58"
      },
      "source": [
        "X_train.shape,X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVJESpf0D126"
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x8LCidoEGBG"
      },
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0.2\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8dgfcK0EUyz",
        "outputId": "ac43ff4c-99b3-4a0c-ebe8-76c7b9d13d5a"
      },
      "source": [
        "model = Model(inputs=input, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 12)   324         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 12)  48          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 12)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 6)    648         ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 32, 32, 6)    0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 18)   0           ['conv2d[0][0]',                 \n",
            "                                                                  'dropout[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 18)  72          ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 18)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 6)    972         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 24)   0           ['concatenate[0][0]',            \n",
            "                                                                  'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 24)  96          ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 24)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 6)    1296        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 30)   0           ['concatenate_1[0][0]',          \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 30)  120         ['concatenate_2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 30)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 6)    1620        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 36)   0           ['concatenate_2[0][0]',          \n",
            "                                                                  'dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 36)  144         ['concatenate_3[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 36)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 6)    1944        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 42)   0           ['concatenate_3[0][0]',          \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 42)  168         ['concatenate_4[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 42)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 6)    2268        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 48)   0           ['concatenate_4[0][0]',          \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 48)  192         ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 48)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 6)    2592        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 54)   0           ['concatenate_5[0][0]',          \n",
            "                                                                  'dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 54)  216         ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 54)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 6)    2916        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 60)   0           ['concatenate_6[0][0]',          \n",
            "                                                                  'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 60)  240         ['concatenate_7[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 32, 60)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 6)    3240        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 32, 32, 66)   0           ['concatenate_7[0][0]',          \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 66)  264         ['concatenate_8[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 32, 66)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 6)    3564        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 32, 32, 6)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate_8[0][0]',          \n",
            "                                                                  'dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 72)  288         ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 32, 72)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 6)    3888        ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 32, 32, 6)    0           ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 32, 32, 78)   0           ['concatenate_9[0][0]',          \n",
            "                                                                  'dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 78)  312         ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 32, 78)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 6)    4212        ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 32, 32, 6)    0           ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 32, 32, 84)   0           ['concatenate_10[0][0]',         \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 32, 84)  336         ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 32, 84)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 6)    504         ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 32, 32, 6)    0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 6)   0           ['dropout_12[0][0]']             \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 6)   24          ['average_pooling2d[0][0]']      \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 6)    0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 6)    324         ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 16, 16, 12)   0           ['average_pooling2d[0][0]',      \n",
            "                                                                  'dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 12)  48          ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 12)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 6)    648         ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 16, 16, 18)   0           ['concatenate_12[0][0]',         \n",
            "                                                                  'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 18)  72          ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 18)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 6)    972         ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 16, 16, 24)   0           ['concatenate_13[0][0]',         \n",
            "                                                                  'dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 24)  96          ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 24)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 6)    1296        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 16, 16, 30)   0           ['concatenate_14[0][0]',         \n",
            "                                                                  'dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 30)  120         ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 30)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 6)    1620        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 16, 16, 36)   0           ['concatenate_15[0][0]',         \n",
            "                                                                  'dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 36)  144         ['concatenate_16[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 6)    1944        ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 16, 16, 42)   0           ['concatenate_16[0][0]',         \n",
            "                                                                  'dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 42)  168         ['concatenate_17[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 42)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 6)    2268        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 16, 16, 48)   0           ['concatenate_17[0][0]',         \n",
            "                                                                  'dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 48)  192         ['concatenate_18[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 6)    2592        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 16, 16, 54)   0           ['concatenate_18[0][0]',         \n",
            "                                                                  'dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 54)  216         ['concatenate_19[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 54)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 6)    2916        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 16, 16, 60)   0           ['concatenate_19[0][0]',         \n",
            "                                                                  'dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 60)  240         ['concatenate_20[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 60)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 6)    3240        ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 16, 16, 66)   0           ['concatenate_20[0][0]',         \n",
            "                                                                  'dropout_22[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 66)  264         ['concatenate_21[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 66)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 6)    3564        ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_21[0][0]',         \n",
            "                                                                  'dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 72)  288         ['concatenate_22[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 6)    3888        ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 16, 16, 78)   0           ['concatenate_22[0][0]',         \n",
            "                                                                  'dropout_24[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 16, 16, 78)  312         ['concatenate_23[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 16, 16, 78)   0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 6)    468         ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 16, 16, 6)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 6)     0           ['dropout_25[0][0]']             \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 6)     24          ['average_pooling2d_1[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 8, 8, 6)      0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 8, 8, 6)      324         ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 8, 8, 12)     0           ['average_pooling2d_1[0][0]',    \n",
            "                                                                  'dropout_26[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 12)    48          ['concatenate_24[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 8, 8, 12)     0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 8, 8, 6)      648         ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 8, 8, 18)     0           ['concatenate_24[0][0]',         \n",
            "                                                                  'dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 18)    72          ['concatenate_25[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 8, 8, 18)     0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 6)      972         ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 8, 8, 24)     0           ['concatenate_25[0][0]',         \n",
            "                                                                  'dropout_28[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 24)    96          ['concatenate_26[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 8, 8, 24)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 6)      1296        ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 8, 8, 30)     0           ['concatenate_26[0][0]',         \n",
            "                                                                  'dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 8, 8, 30)    120         ['concatenate_27[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 8, 8, 30)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 6)      1620        ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 8, 8, 36)     0           ['concatenate_27[0][0]',         \n",
            "                                                                  'dropout_30[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 8, 8, 36)    144         ['concatenate_28[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 6)      1944        ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 8, 8, 42)     0           ['concatenate_28[0][0]',         \n",
            "                                                                  'dropout_31[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 8, 8, 42)    168         ['concatenate_29[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 8, 8, 42)     0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 6)      2268        ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 8, 8, 48)     0           ['concatenate_29[0][0]',         \n",
            "                                                                  'dropout_32[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 8, 8, 48)    192         ['concatenate_30[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 8, 8, 48)     0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 8, 8, 6)      2592        ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 8, 8, 54)     0           ['concatenate_30[0][0]',         \n",
            "                                                                  'dropout_33[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 8, 8, 54)    216         ['concatenate_31[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 8, 8, 54)     0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 8, 8, 6)      2916        ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 8, 8, 60)     0           ['concatenate_31[0][0]',         \n",
            "                                                                  'dropout_34[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 8, 8, 60)    240         ['concatenate_32[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 8, 8, 6)      3240        ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 8, 8, 66)     0           ['concatenate_32[0][0]',         \n",
            "                                                                  'dropout_35[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 8, 8, 66)    264         ['concatenate_33[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 8, 8, 66)     0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 8, 8, 6)      3564        ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_33[0][0]',         \n",
            "                                                                  'dropout_36[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 8, 8, 72)    288         ['concatenate_34[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 8, 8, 6)      3888        ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 8, 8, 78)     0           ['concatenate_34[0][0]',         \n",
            "                                                                  'dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 8, 8, 78)    312         ['concatenate_35[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 8, 8, 78)     0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 8, 8, 6)      468         ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 8, 8, 6)      0           ['conv2d_39[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 6)     0           ['dropout_38[0][0]']             \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 4, 4, 6)     24          ['average_pooling2d_2[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 4, 4, 6)      0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 4, 4, 6)      324         ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 4, 4, 12)     0           ['average_pooling2d_2[0][0]',    \n",
            "                                                                  'dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 4, 4, 12)    48          ['concatenate_36[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 4, 4, 12)     0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 4, 4, 6)      648         ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 4, 4, 18)     0           ['concatenate_36[0][0]',         \n",
            "                                                                  'dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 4, 4, 18)    72          ['concatenate_37[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 4, 4, 18)     0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 4, 4, 6)      972         ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 4, 4, 24)     0           ['concatenate_37[0][0]',         \n",
            "                                                                  'dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 4, 4, 24)    96          ['concatenate_38[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 4, 4, 24)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 4, 4, 6)      1296        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 4, 4, 30)     0           ['concatenate_38[0][0]',         \n",
            "                                                                  'dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 4, 4, 30)    120         ['concatenate_39[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 4, 4, 30)     0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 4, 4, 6)      1620        ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 4, 4, 36)     0           ['concatenate_39[0][0]',         \n",
            "                                                                  'dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 4, 4, 36)    144         ['concatenate_40[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 4, 4, 6)      1944        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 4, 4, 42)     0           ['concatenate_40[0][0]',         \n",
            "                                                                  'dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 4, 4, 42)    168         ['concatenate_41[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 4, 4, 42)     0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 4, 4, 6)      2268        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 4, 4, 48)     0           ['concatenate_41[0][0]',         \n",
            "                                                                  'dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 4, 4, 48)    192         ['concatenate_42[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 4, 4, 48)     0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 4, 4, 6)      2592        ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 4, 4, 54)     0           ['concatenate_42[0][0]',         \n",
            "                                                                  'dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 4, 4, 54)    216         ['concatenate_43[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 4, 4, 54)     0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 4, 4, 6)      2916        ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 4, 4, 60)     0           ['concatenate_43[0][0]',         \n",
            "                                                                  'dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 4, 4, 60)    240         ['concatenate_44[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 4, 4, 60)     0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 4, 4, 6)      3240        ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 4, 4, 66)     0           ['concatenate_44[0][0]',         \n",
            "                                                                  'dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 4, 4, 66)    264         ['concatenate_45[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 4, 4, 66)     0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 4, 4, 6)      3564        ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_45[0][0]',         \n",
            "                                                                  'dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 4, 4, 72)    288         ['concatenate_46[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 4, 4, 6)      3888        ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 4, 4, 6)      0           ['conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 4, 4, 78)     0           ['concatenate_46[0][0]',         \n",
            "                                                                  'dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 4, 4, 78)    312         ['concatenate_47[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 4, 4, 78)     0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 2, 2, 78)    0           ['activation_51[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 312)          0           ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           3130        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 118,918\n",
            "Trainable params: 114,394\n",
            "Non-trainable params: 4,524\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqNsxocCEd0b"
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOlby-EkEnLy",
        "outputId": "32100d75-5a11-498c-bd84-24b934534a01"
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 129s 229ms/step - loss: 1.7648 - accuracy: 0.3471 - val_loss: 2.0367 - val_accuracy: 0.3052\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 1.4214 - accuracy: 0.4739 - val_loss: 1.5315 - val_accuracy: 0.4604\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 1.2394 - accuracy: 0.5472 - val_loss: 1.5906 - val_accuracy: 0.4441\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 1.1207 - accuracy: 0.5971 - val_loss: 1.4406 - val_accuracy: 0.5141\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 1.0459 - accuracy: 0.6244 - val_loss: 1.2323 - val_accuracy: 0.5900\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.9907 - accuracy: 0.6428 - val_loss: 1.0826 - val_accuracy: 0.6264\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.9554 - accuracy: 0.6562 - val_loss: 1.0039 - val_accuracy: 0.6530\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 85s 219ms/step - loss: 0.9188 - accuracy: 0.6681 - val_loss: 1.1784 - val_accuracy: 0.6177\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.8923 - accuracy: 0.6803 - val_loss: 1.1247 - val_accuracy: 0.6271\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.8702 - accuracy: 0.6859 - val_loss: 0.9761 - val_accuracy: 0.6718\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.8454 - accuracy: 0.6994 - val_loss: 1.1651 - val_accuracy: 0.6101\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.8289 - accuracy: 0.7027 - val_loss: 0.9153 - val_accuracy: 0.6828\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.8112 - accuracy: 0.7086 - val_loss: 0.9363 - val_accuracy: 0.6762\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.7902 - accuracy: 0.7171 - val_loss: 0.9984 - val_accuracy: 0.6724\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.7763 - accuracy: 0.7229 - val_loss: 0.8410 - val_accuracy: 0.7096\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.7594 - accuracy: 0.7289 - val_loss: 0.7800 - val_accuracy: 0.7300\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.7472 - accuracy: 0.7335 - val_loss: 0.8649 - val_accuracy: 0.7091\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.7370 - accuracy: 0.7385 - val_loss: 0.9528 - val_accuracy: 0.6898\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.7190 - accuracy: 0.7446 - val_loss: 0.7927 - val_accuracy: 0.7305\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.7088 - accuracy: 0.7490 - val_loss: 0.8128 - val_accuracy: 0.7286\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6981 - accuracy: 0.7505 - val_loss: 0.8701 - val_accuracy: 0.7196\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6880 - accuracy: 0.7568 - val_loss: 0.7976 - val_accuracy: 0.7339\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6782 - accuracy: 0.7566 - val_loss: 0.8769 - val_accuracy: 0.7074\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6669 - accuracy: 0.7614 - val_loss: 0.8149 - val_accuracy: 0.7345\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.6600 - accuracy: 0.7665 - val_loss: 0.8387 - val_accuracy: 0.7294\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6511 - accuracy: 0.7706 - val_loss: 0.8727 - val_accuracy: 0.7236\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6406 - accuracy: 0.7734 - val_loss: 0.9046 - val_accuracy: 0.7150\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6303 - accuracy: 0.7780 - val_loss: 0.8726 - val_accuracy: 0.7203\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6240 - accuracy: 0.7790 - val_loss: 0.7683 - val_accuracy: 0.7501\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 85s 219ms/step - loss: 0.6201 - accuracy: 0.7818 - val_loss: 0.7463 - val_accuracy: 0.7572\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6098 - accuracy: 0.7855 - val_loss: 1.0804 - val_accuracy: 0.6855\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.6065 - accuracy: 0.7851 - val_loss: 0.7723 - val_accuracy: 0.7543\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5960 - accuracy: 0.7873 - val_loss: 0.8693 - val_accuracy: 0.7323\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 85s 219ms/step - loss: 0.5932 - accuracy: 0.7884 - val_loss: 0.8485 - val_accuracy: 0.7395\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5909 - accuracy: 0.7899 - val_loss: 0.7559 - val_accuracy: 0.7582\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5789 - accuracy: 0.7938 - val_loss: 0.8917 - val_accuracy: 0.7226\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5733 - accuracy: 0.7961 - val_loss: 0.6789 - val_accuracy: 0.7761\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5720 - accuracy: 0.7976 - val_loss: 0.7305 - val_accuracy: 0.7682\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5662 - accuracy: 0.7993 - val_loss: 0.7970 - val_accuracy: 0.7536\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 85s 217ms/step - loss: 0.5625 - accuracy: 0.8018 - val_loss: 0.7668 - val_accuracy: 0.7565\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5579 - accuracy: 0.8010 - val_loss: 0.7348 - val_accuracy: 0.7650\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5548 - accuracy: 0.8028 - val_loss: 0.9156 - val_accuracy: 0.7308\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5486 - accuracy: 0.8040 - val_loss: 0.7729 - val_accuracy: 0.7601\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 85s 217ms/step - loss: 0.5452 - accuracy: 0.8073 - val_loss: 0.7305 - val_accuracy: 0.7752\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 85s 218ms/step - loss: 0.5380 - accuracy: 0.8098 - val_loss: 0.9094 - val_accuracy: 0.7390\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 85s 217ms/step - loss: 0.5398 - accuracy: 0.8066 - val_loss: 0.7962 - val_accuracy: 0.7505\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.5322 - accuracy: 0.8108 - val_loss: 0.9713 - val_accuracy: 0.7168\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.5258 - accuracy: 0.8124 - val_loss: 0.7473 - val_accuracy: 0.7714\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.5268 - accuracy: 0.8146 - val_loss: 0.8050 - val_accuracy: 0.7548\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.5199 - accuracy: 0.8170 - val_loss: 0.7435 - val_accuracy: 0.7718\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.5164 - accuracy: 0.8172 - val_loss: 0.7243 - val_accuracy: 0.7751\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.5159 - accuracy: 0.8174 - val_loss: 0.7311 - val_accuracy: 0.7785\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.5114 - accuracy: 0.8180 - val_loss: 0.7407 - val_accuracy: 0.7760\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.5061 - accuracy: 0.8205 - val_loss: 0.8639 - val_accuracy: 0.7465\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.5077 - accuracy: 0.8190 - val_loss: 0.7285 - val_accuracy: 0.7739\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.5051 - accuracy: 0.8210 - val_loss: 0.8441 - val_accuracy: 0.7493\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4951 - accuracy: 0.8248 - val_loss: 0.7339 - val_accuracy: 0.7732\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4992 - accuracy: 0.8211 - val_loss: 0.6801 - val_accuracy: 0.7914\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.4961 - accuracy: 0.8239 - val_loss: 0.7331 - val_accuracy: 0.7779\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4915 - accuracy: 0.8257 - val_loss: 0.8013 - val_accuracy: 0.7543\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4903 - accuracy: 0.8262 - val_loss: 0.7067 - val_accuracy: 0.7828\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 86s 219ms/step - loss: 0.4848 - accuracy: 0.8283 - val_loss: 0.6861 - val_accuracy: 0.7884\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4813 - accuracy: 0.8286 - val_loss: 0.6765 - val_accuracy: 0.7919\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4790 - accuracy: 0.8294 - val_loss: 0.7730 - val_accuracy: 0.7683\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4833 - accuracy: 0.8282 - val_loss: 0.7319 - val_accuracy: 0.7742\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4773 - accuracy: 0.8320 - val_loss: 0.6697 - val_accuracy: 0.7904\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4716 - accuracy: 0.8323 - val_loss: 0.7727 - val_accuracy: 0.7742\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4738 - accuracy: 0.8316 - val_loss: 0.8614 - val_accuracy: 0.7593\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4745 - accuracy: 0.8316 - val_loss: 0.7136 - val_accuracy: 0.7853\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4709 - accuracy: 0.8325 - val_loss: 0.7508 - val_accuracy: 0.7747\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4685 - accuracy: 0.8330 - val_loss: 0.6759 - val_accuracy: 0.7960\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4640 - accuracy: 0.8363 - val_loss: 0.6728 - val_accuracy: 0.7927\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4630 - accuracy: 0.8336 - val_loss: 0.7243 - val_accuracy: 0.7804\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4610 - accuracy: 0.8370 - val_loss: 0.6733 - val_accuracy: 0.7966\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4577 - accuracy: 0.8372 - val_loss: 0.7792 - val_accuracy: 0.7731\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4552 - accuracy: 0.8379 - val_loss: 0.7686 - val_accuracy: 0.7812\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4575 - accuracy: 0.8378 - val_loss: 0.6470 - val_accuracy: 0.8016\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4506 - accuracy: 0.8397 - val_loss: 0.9279 - val_accuracy: 0.7488\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4523 - accuracy: 0.8399 - val_loss: 0.7414 - val_accuracy: 0.7800\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4510 - accuracy: 0.8390 - val_loss: 0.8113 - val_accuracy: 0.7703\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4498 - accuracy: 0.8408 - val_loss: 0.6209 - val_accuracy: 0.8101\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4439 - accuracy: 0.8414 - val_loss: 0.6981 - val_accuracy: 0.7951\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4456 - accuracy: 0.8410 - val_loss: 0.6779 - val_accuracy: 0.7937\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4411 - accuracy: 0.8398 - val_loss: 0.8391 - val_accuracy: 0.7564\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4407 - accuracy: 0.8426 - val_loss: 0.8133 - val_accuracy: 0.7744\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4383 - accuracy: 0.8444 - val_loss: 0.7123 - val_accuracy: 0.7883\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4381 - accuracy: 0.8441 - val_loss: 0.7465 - val_accuracy: 0.7866\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4344 - accuracy: 0.8450 - val_loss: 0.7508 - val_accuracy: 0.7861\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4380 - accuracy: 0.8431 - val_loss: 0.6826 - val_accuracy: 0.7983\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4320 - accuracy: 0.8459 - val_loss: 0.7054 - val_accuracy: 0.7957\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4310 - accuracy: 0.8472 - val_loss: 0.6997 - val_accuracy: 0.7991\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4326 - accuracy: 0.8474 - val_loss: 0.7399 - val_accuracy: 0.7810\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4281 - accuracy: 0.8474 - val_loss: 0.6930 - val_accuracy: 0.7993\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4326 - accuracy: 0.8457 - val_loss: 0.6890 - val_accuracy: 0.7957\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4272 - accuracy: 0.8471 - val_loss: 0.6963 - val_accuracy: 0.7976\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4255 - accuracy: 0.8478 - val_loss: 0.6455 - val_accuracy: 0.8029\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4264 - accuracy: 0.8491 - val_loss: 0.7076 - val_accuracy: 0.7949\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4219 - accuracy: 0.8512 - val_loss: 0.7209 - val_accuracy: 0.7898\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4224 - accuracy: 0.8518 - val_loss: 0.8143 - val_accuracy: 0.7663\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4163 - accuracy: 0.8529 - val_loss: 0.6988 - val_accuracy: 0.7996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3c50182690>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP40d8wq40nC"
      },
      "source": [
        "model.save(\"model_train.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3oiQuu5GFv"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/model_train.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6Z0JKrN5NM5",
        "outputId": "f3be26c3-a449-4f80-db80-dd4469ee9fba"
      },
      "source": [
        "#start with last epochs \n",
        "model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 93s 225ms/step - loss: 0.4221 - accuracy: 0.8498 - val_loss: 0.6362 - val_accuracy: 0.8100\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4165 - accuracy: 0.8526 - val_loss: 0.6657 - val_accuracy: 0.8079\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4163 - accuracy: 0.8517 - val_loss: 0.6580 - val_accuracy: 0.8076\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4158 - accuracy: 0.8526 - val_loss: 0.6126 - val_accuracy: 0.8142\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4143 - accuracy: 0.8532 - val_loss: 0.7632 - val_accuracy: 0.7747\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4142 - accuracy: 0.8537 - val_loss: 0.6562 - val_accuracy: 0.8030\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.4122 - accuracy: 0.8537 - val_loss: 0.6642 - val_accuracy: 0.8027\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4124 - accuracy: 0.8533 - val_loss: 0.6485 - val_accuracy: 0.8159\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4108 - accuracy: 0.8526 - val_loss: 0.6114 - val_accuracy: 0.8121\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4079 - accuracy: 0.8544 - val_loss: 0.6721 - val_accuracy: 0.8097\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4088 - accuracy: 0.8536 - val_loss: 0.6649 - val_accuracy: 0.8084\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4044 - accuracy: 0.8542 - val_loss: 0.7450 - val_accuracy: 0.7872\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4026 - accuracy: 0.8572 - val_loss: 0.7170 - val_accuracy: 0.8010\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4103 - accuracy: 0.8553 - val_loss: 0.6308 - val_accuracy: 0.8155\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.4038 - accuracy: 0.8574 - val_loss: 0.6455 - val_accuracy: 0.8112\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4007 - accuracy: 0.8564 - val_loss: 0.6446 - val_accuracy: 0.8090\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4024 - accuracy: 0.8577 - val_loss: 0.6773 - val_accuracy: 0.8001\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.4027 - accuracy: 0.8570 - val_loss: 0.6319 - val_accuracy: 0.8164\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4019 - accuracy: 0.8561 - val_loss: 0.7574 - val_accuracy: 0.7845\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3994 - accuracy: 0.8579 - val_loss: 0.6510 - val_accuracy: 0.8111\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.4001 - accuracy: 0.8565 - val_loss: 0.7366 - val_accuracy: 0.7902\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3946 - accuracy: 0.8584 - val_loss: 0.6451 - val_accuracy: 0.8102\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3929 - accuracy: 0.8592 - val_loss: 0.6549 - val_accuracy: 0.8098\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3963 - accuracy: 0.8592 - val_loss: 0.6116 - val_accuracy: 0.8132\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3935 - accuracy: 0.8600 - val_loss: 0.6052 - val_accuracy: 0.8185\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3904 - accuracy: 0.8604 - val_loss: 0.6659 - val_accuracy: 0.8102\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3939 - accuracy: 0.8604 - val_loss: 0.7053 - val_accuracy: 0.7968\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3859 - accuracy: 0.8628 - val_loss: 0.6212 - val_accuracy: 0.8211\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 87s 224ms/step - loss: 0.3883 - accuracy: 0.8618 - val_loss: 0.7141 - val_accuracy: 0.8008\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3914 - accuracy: 0.8596 - val_loss: 0.5831 - val_accuracy: 0.8279\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3879 - accuracy: 0.8626 - val_loss: 0.6874 - val_accuracy: 0.8028\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3829 - accuracy: 0.8642 - val_loss: 0.6400 - val_accuracy: 0.8167\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.3840 - accuracy: 0.8637 - val_loss: 0.6345 - val_accuracy: 0.8090\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.3859 - accuracy: 0.8621 - val_loss: 0.6389 - val_accuracy: 0.8116\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3872 - accuracy: 0.8614 - val_loss: 0.6754 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3844 - accuracy: 0.8633 - val_loss: 0.6321 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3814 - accuracy: 0.8653 - val_loss: 0.6646 - val_accuracy: 0.8115\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3802 - accuracy: 0.8654 - val_loss: 0.6703 - val_accuracy: 0.8101\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3854 - accuracy: 0.8622 - val_loss: 0.6702 - val_accuracy: 0.8080\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3798 - accuracy: 0.8641 - val_loss: 0.5979 - val_accuracy: 0.8222\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3827 - accuracy: 0.8639 - val_loss: 0.5881 - val_accuracy: 0.8253\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3806 - accuracy: 0.8641 - val_loss: 0.6431 - val_accuracy: 0.8141\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3759 - accuracy: 0.8663 - val_loss: 0.6486 - val_accuracy: 0.8116\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 88s 224ms/step - loss: 0.3759 - accuracy: 0.8645 - val_loss: 0.6502 - val_accuracy: 0.8127\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.8826 - val_accuracy: 0.7595\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3744 - accuracy: 0.8658 - val_loss: 0.6129 - val_accuracy: 0.8190\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 88s 224ms/step - loss: 0.3770 - accuracy: 0.8645 - val_loss: 0.6173 - val_accuracy: 0.8242\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3731 - accuracy: 0.8660 - val_loss: 0.6716 - val_accuracy: 0.8119\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3751 - accuracy: 0.8651 - val_loss: 0.6825 - val_accuracy: 0.8079\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3784 - accuracy: 0.8655 - val_loss: 0.6339 - val_accuracy: 0.8234\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3711 - accuracy: 0.8680 - val_loss: 0.6406 - val_accuracy: 0.8150\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3743 - accuracy: 0.8663 - val_loss: 0.6129 - val_accuracy: 0.8202\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3669 - accuracy: 0.8687 - val_loss: 0.6392 - val_accuracy: 0.8170\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3685 - accuracy: 0.8679 - val_loss: 0.6577 - val_accuracy: 0.8167\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3716 - accuracy: 0.8681 - val_loss: 0.6856 - val_accuracy: 0.8124\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3682 - accuracy: 0.8692 - val_loss: 0.5848 - val_accuracy: 0.8235\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3650 - accuracy: 0.8714 - val_loss: 0.6211 - val_accuracy: 0.8166\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3698 - accuracy: 0.8674 - val_loss: 0.6410 - val_accuracy: 0.8194\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3693 - accuracy: 0.8691 - val_loss: 0.6341 - val_accuracy: 0.8197\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3685 - accuracy: 0.8695 - val_loss: 0.5947 - val_accuracy: 0.8255\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3656 - accuracy: 0.8706 - val_loss: 0.6529 - val_accuracy: 0.8104\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3636 - accuracy: 0.8690 - val_loss: 0.5942 - val_accuracy: 0.8273\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3644 - accuracy: 0.8698 - val_loss: 0.6926 - val_accuracy: 0.8070\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3679 - accuracy: 0.8676 - val_loss: 0.6326 - val_accuracy: 0.8163\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3620 - accuracy: 0.8698 - val_loss: 0.5948 - val_accuracy: 0.8262\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3629 - accuracy: 0.8690 - val_loss: 0.5905 - val_accuracy: 0.8285\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3651 - accuracy: 0.8700 - val_loss: 0.6279 - val_accuracy: 0.8218\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3592 - accuracy: 0.8710 - val_loss: 0.6441 - val_accuracy: 0.8171\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3576 - accuracy: 0.8727 - val_loss: 0.6067 - val_accuracy: 0.8244\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3618 - accuracy: 0.8706 - val_loss: 0.6207 - val_accuracy: 0.8258\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3643 - accuracy: 0.8701 - val_loss: 0.6757 - val_accuracy: 0.8041\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3567 - accuracy: 0.8719 - val_loss: 0.6244 - val_accuracy: 0.8246\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3589 - accuracy: 0.8722 - val_loss: 0.6206 - val_accuracy: 0.8251\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3582 - accuracy: 0.8724 - val_loss: 0.6949 - val_accuracy: 0.8129\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3578 - accuracy: 0.8728 - val_loss: 0.6111 - val_accuracy: 0.8211\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3567 - accuracy: 0.8726 - val_loss: 0.6568 - val_accuracy: 0.8140\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3570 - accuracy: 0.8725 - val_loss: 0.5759 - val_accuracy: 0.8299\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.3551 - accuracy: 0.8726 - val_loss: 0.6380 - val_accuracy: 0.8167\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 86s 220ms/step - loss: 0.3582 - accuracy: 0.8731 - val_loss: 0.6144 - val_accuracy: 0.8186\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3560 - accuracy: 0.8707 - val_loss: 0.6244 - val_accuracy: 0.8214\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3574 - accuracy: 0.8720 - val_loss: 0.5969 - val_accuracy: 0.8258\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3522 - accuracy: 0.8729 - val_loss: 0.6441 - val_accuracy: 0.8195\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3547 - accuracy: 0.8738 - val_loss: 0.5651 - val_accuracy: 0.8355\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3543 - accuracy: 0.8729 - val_loss: 0.7698 - val_accuracy: 0.7887\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3521 - accuracy: 0.8745 - val_loss: 0.6462 - val_accuracy: 0.8178\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3492 - accuracy: 0.8758 - val_loss: 0.6383 - val_accuracy: 0.8175\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 86s 221ms/step - loss: 0.3500 - accuracy: 0.8751 - val_loss: 0.6287 - val_accuracy: 0.8236\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3505 - accuracy: 0.8753 - val_loss: 0.6350 - val_accuracy: 0.8193\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 87s 221ms/step - loss: 0.3482 - accuracy: 0.8759 - val_loss: 0.6359 - val_accuracy: 0.8154\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3498 - accuracy: 0.8758 - val_loss: 0.6118 - val_accuracy: 0.8258\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3484 - accuracy: 0.8765 - val_loss: 0.6509 - val_accuracy: 0.8163\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3491 - accuracy: 0.8751 - val_loss: 0.6135 - val_accuracy: 0.8293\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3488 - accuracy: 0.8740 - val_loss: 0.6686 - val_accuracy: 0.8203\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3486 - accuracy: 0.8757 - val_loss: 0.6749 - val_accuracy: 0.8100\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 87s 222ms/step - loss: 0.3483 - accuracy: 0.8747 - val_loss: 0.6275 - val_accuracy: 0.8241\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 87s 223ms/step - loss: 0.3448 - accuracy: 0.8770 - val_loss: 0.6588 - val_accuracy: 0.8201\n",
            "Epoch 97/100\n",
            "354/391 [==========================>...] - ETA: 7s - loss: 0.3492 - accuracy: 0.8729"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA-hm8OeT6Wy"
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azPQ2daqU9Qc"
      },
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (5,5), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, 32, dropout_rate)\n",
        "First_Transition = transition(First_Block, 32, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, 16, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, 16, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, 16, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, 8, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  8, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j6wkt7iVMQc",
        "outputId": "319068b2-e6ff-451d-843e-6f0f55ed228c"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model_2 = Model(inputs = input,outputs = output)\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 12)   900         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 12)  48          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 12)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 16)   4800        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 28)   0           ['conv2d[0][0]',                 \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 28)  112         ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 28)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 16)   11200       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 44)   0           ['concatenate[0][0]',            \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 44)  176         ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 44)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 16)   17600       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 60)   0           ['concatenate_1[0][0]',          \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 60)  240         ['concatenate_2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 60)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 16)   24000       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 76)   0           ['concatenate_2[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 76)  304         ['concatenate_3[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 76)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 16)   30400       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 92)   0           ['concatenate_3[0][0]',          \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 92)  368         ['concatenate_4[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 92)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 16)   36800       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_4[0][0]',          \n",
            "                                                                  'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 16)   43200       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 124)  0           ['concatenate_5[0][0]',          \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 124)  496        ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 124)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 16)   49600       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 140)  0           ['concatenate_6[0][0]',          \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 140)  560        ['concatenate_7[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 32, 140)  0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 16)   56000       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 32, 32, 156)  0           ['concatenate_7[0][0]',          \n",
            "                                                                  'conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 156)  624        ['concatenate_8[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 32, 156)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 16)   62400       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 32, 32, 172)  0           ['concatenate_8[0][0]',          \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 172)  688        ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 32, 172)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 16)   68800       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 32, 32, 188)  0           ['concatenate_9[0][0]',          \n",
            "                                                                  'conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 188)  752        ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 32, 188)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 16)   75200       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 32, 32, 204)  0           ['concatenate_10[0][0]',         \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 32, 204)  816        ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 32, 204)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 16)   3264        ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 16)  0           ['conv2d_13[0][0]']              \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 16)  64          ['average_pooling2d[0][0]']      \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 16)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 8)    3200        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 16, 16, 24)   0           ['average_pooling2d[0][0]',      \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 24)  96          ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 24)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 8)    4800        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 16, 16, 32)   0           ['concatenate_12[0][0]',         \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 8)    6400        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 16, 16, 40)   0           ['concatenate_13[0][0]',         \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 40)  160         ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 40)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 8)    8000        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 16, 16, 48)   0           ['concatenate_14[0][0]',         \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 48)  192         ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 8)    9600        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 16, 16, 56)   0           ['concatenate_15[0][0]',         \n",
            "                                                                  'conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 56)  224         ['concatenate_16[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 56)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 8)    11200       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 16, 16, 64)   0           ['concatenate_16[0][0]',         \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 64)  256         ['concatenate_17[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 8)    12800       ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_17[0][0]',         \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 72)  288         ['concatenate_18[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 8)    14400       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 16, 16, 80)   0           ['concatenate_18[0][0]',         \n",
            "                                                                  'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 80)  320         ['concatenate_19[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 80)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 8)    16000       ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 16, 16, 88)   0           ['concatenate_19[0][0]',         \n",
            "                                                                  'conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 88)  352         ['concatenate_20[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 88)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 8)    17600       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 16, 16, 96)   0           ['concatenate_20[0][0]',         \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 96)  384         ['concatenate_21[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 8)    19200       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 16, 16, 104)  0           ['concatenate_21[0][0]',         \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 104)  416        ['concatenate_22[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 104)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 8)    20800       ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 16, 16, 112)  0           ['concatenate_22[0][0]',         \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 16, 16, 112)  448        ['concatenate_23[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 16, 16, 112)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 8)    896         ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 8)     0           ['conv2d_26[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 8)     32          ['average_pooling2d_1[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 8, 8, 8)      0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 8, 8, 8)      1600        ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 8, 8, 16)     0           ['average_pooling2d_1[0][0]',    \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 16)    64          ['concatenate_24[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 8, 8, 16)     0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 8, 8, 8)      3200        ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 8, 8, 24)     0           ['concatenate_24[0][0]',         \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 24)    96          ['concatenate_25[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 8, 8, 24)     0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 8)      4800        ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 8, 8, 32)     0           ['concatenate_25[0][0]',         \n",
            "                                                                  'conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 32)    128         ['concatenate_26[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 8, 8, 32)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 8)      6400        ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 8, 8, 40)     0           ['concatenate_26[0][0]',         \n",
            "                                                                  'conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 8, 8, 40)    160         ['concatenate_27[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 8, 8, 40)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 8)      8000        ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 8, 8, 48)     0           ['concatenate_27[0][0]',         \n",
            "                                                                  'conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 8, 8, 48)    192         ['concatenate_28[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 8, 8, 48)     0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 8)      9600        ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 8, 8, 56)     0           ['concatenate_28[0][0]',         \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 8, 8, 56)    224         ['concatenate_29[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 8, 8, 56)     0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 8)      11200       ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 8, 8, 64)     0           ['concatenate_29[0][0]',         \n",
            "                                                                  'conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 8, 8, 64)    256         ['concatenate_30[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 8, 8, 8)      12800       ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_30[0][0]',         \n",
            "                                                                  'conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 8, 8, 72)    288         ['concatenate_31[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 8, 8, 8)      14400       ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 8, 8, 80)     0           ['concatenate_31[0][0]',         \n",
            "                                                                  'conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 8, 8, 80)    320         ['concatenate_32[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 8, 8, 80)     0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 8, 8, 8)      16000       ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 8, 8, 88)     0           ['concatenate_32[0][0]',         \n",
            "                                                                  'conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 8, 8, 88)    352         ['concatenate_33[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 8, 8, 88)     0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 8, 8, 8)      17600       ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 8, 8, 96)     0           ['concatenate_33[0][0]',         \n",
            "                                                                  'conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 8, 8, 96)    384         ['concatenate_34[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 8, 8, 96)     0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 8, 8, 8)      19200       ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 8, 8, 104)    0           ['concatenate_34[0][0]',         \n",
            "                                                                  'conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 8, 8, 104)   416         ['concatenate_35[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 8, 8, 104)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 8, 8, 4)      416         ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 4)     0           ['conv2d_39[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 4, 4, 4)     16          ['average_pooling2d_2[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 4, 4, 4)      0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 4, 4, 4)      400         ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 4, 4, 8)      0           ['average_pooling2d_2[0][0]',    \n",
            "                                                                  'conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 4, 4, 8)     32          ['concatenate_36[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 4, 4, 8)      0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 4, 4, 4)      800         ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 4, 4, 12)     0           ['concatenate_36[0][0]',         \n",
            "                                                                  'conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 4, 4, 12)    48          ['concatenate_37[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 4, 4, 12)     0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 4, 4, 4)      1200        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 4, 4, 16)     0           ['concatenate_37[0][0]',         \n",
            "                                                                  'conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 4, 4, 16)    64          ['concatenate_38[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 4, 4, 16)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 4, 4, 4)      1600        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 4, 4, 20)     0           ['concatenate_38[0][0]',         \n",
            "                                                                  'conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 4, 4, 20)    80          ['concatenate_39[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 4, 4, 20)     0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 4, 4, 4)      2000        ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 4, 4, 24)     0           ['concatenate_39[0][0]',         \n",
            "                                                                  'conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 4, 4, 24)    96          ['concatenate_40[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 4, 4, 24)     0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 4, 4, 4)      2400        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 4, 4, 28)     0           ['concatenate_40[0][0]',         \n",
            "                                                                  'conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 4, 4, 28)    112         ['concatenate_41[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 4, 4, 28)     0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 4, 4, 4)      2800        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 4, 4, 32)     0           ['concatenate_41[0][0]',         \n",
            "                                                                  'conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 4, 4, 32)    128         ['concatenate_42[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 4, 4, 32)     0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 4, 4, 4)      3200        ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 4, 4, 36)     0           ['concatenate_42[0][0]',         \n",
            "                                                                  'conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 4, 4, 36)    144         ['concatenate_43[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 4, 4, 4)      3600        ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 4, 4, 40)     0           ['concatenate_43[0][0]',         \n",
            "                                                                  'conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 4, 4, 40)    160         ['concatenate_44[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 4, 4, 40)     0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 4, 4, 4)      4000        ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 4, 4, 44)     0           ['concatenate_44[0][0]',         \n",
            "                                                                  'conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 4, 4, 44)    176         ['concatenate_45[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 4, 4, 44)     0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 4, 4, 4)      4400        ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 4, 4, 48)     0           ['concatenate_45[0][0]',         \n",
            "                                                                  'conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 4, 4, 48)    192         ['concatenate_46[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 4, 4, 48)     0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 4, 4, 4)      4800        ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 4, 4, 52)     0           ['concatenate_46[0][0]',         \n",
            "                                                                  'conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 4, 4, 52)    208         ['concatenate_47[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 4, 4, 52)     0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 2, 2, 52)    0           ['activation_51[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 208)          0           ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           2090        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 800,878\n",
            "Trainable params: 794,222\n",
            "Non-trainable params: 6,656\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htME3vUgTyKJ"
      },
      "source": [
        "def preprocess_image(x_train,x_test):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    x_train --> training dataframe - type pandas -core-DataFrame\n",
        "\n",
        "    x_test --> test dataframe - type pandas -core-DataFrame\n",
        "\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  #preprocess the image scaling\n",
        "  #Reference : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "  train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "                samplewise_center=False,  # set each sample mean to 0\n",
        "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "                samplewise_std_normalization=False,  # divide each input by its std\n",
        "                zca_whitening=False,  # apply ZCA whitening\n",
        "                rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                zoom_range = 0.2, # Randomly zoom image \n",
        "                width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "                height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "                horizontal_flip=True,  # randomly flip images\n",
        "                vertical_flip=False) \n",
        "  \n",
        "\n",
        "  train_generator.fit(x_train)\n",
        "\n",
        " \n",
        "  validation_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  validation_generator.fit(x_test)\n",
        "\n",
        "  return train_generator,validation_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBOLeGHOX7hC"
      },
      "source": [
        "train_generator,validation_generator = preprocess_image(X_train,X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNlhJ9YNYGCB",
        "outputId": "7ade4d01-b913-4412-ae36-ef96a6d28190"
      },
      "source": [
        "import datetime\n",
        "batch_size = 512\n",
        "filepath=\"Best_weight-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "\n",
        "log_dir = \"CnnCipher10\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "                            log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_2.fit(train_generator.flow(X_train,y_train,batch_size= batch_size),\n",
        "            epochs= 100,verbose= 1,validation_data=validation_generator.flow(X_test,y_test,batch_size = 256),\n",
        "            callbacks = [cp_callback,tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.8671 - accuracy: 0.3007\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.10450, saving model to Best_weight-01-0.10.hdf5\n",
            "98/98 [==============================] - 291s 2s/step - loss: 1.8671 - accuracy: 0.3007 - val_loss: 3.2265 - val_accuracy: 0.1045\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.5904 - accuracy: 0.4140\n",
            "Epoch 00002: val_accuracy improved from 0.10450 to 0.17500, saving model to Best_weight-02-0.17.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.5904 - accuracy: 0.4140 - val_loss: 2.6548 - val_accuracy: 0.1750\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.4609 - accuracy: 0.4692\n",
            "Epoch 00003: val_accuracy improved from 0.17500 to 0.31360, saving model to Best_weight-03-0.31.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.4609 - accuracy: 0.4692 - val_loss: 1.9886 - val_accuracy: 0.3136\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.3523 - accuracy: 0.5087\n",
            "Epoch 00004: val_accuracy improved from 0.31360 to 0.47120, saving model to Best_weight-04-0.47.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.3523 - accuracy: 0.5087 - val_loss: 1.4569 - val_accuracy: 0.4712\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.2775 - accuracy: 0.5378\n",
            "Epoch 00005: val_accuracy improved from 0.47120 to 0.54530, saving model to Best_weight-05-0.55.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.2775 - accuracy: 0.5378 - val_loss: 1.2947 - val_accuracy: 0.5453\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.1941 - accuracy: 0.5725\n",
            "Epoch 00006: val_accuracy improved from 0.54530 to 0.58170, saving model to Best_weight-06-0.58.hdf5\n",
            "98/98 [==============================] - 223s 2s/step - loss: 1.1941 - accuracy: 0.5725 - val_loss: 1.1886 - val_accuracy: 0.5817\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.1396 - accuracy: 0.5906\n",
            "Epoch 00007: val_accuracy improved from 0.58170 to 0.58210, saving model to Best_weight-07-0.58.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.1396 - accuracy: 0.5906 - val_loss: 1.2143 - val_accuracy: 0.5821\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0955 - accuracy: 0.6092\n",
            "Epoch 00008: val_accuracy did not improve from 0.58210\n",
            "98/98 [==============================] - 215s 2s/step - loss: 1.0955 - accuracy: 0.6092 - val_loss: 1.3430 - val_accuracy: 0.5669\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0404 - accuracy: 0.6305\n",
            "Epoch 00009: val_accuracy improved from 0.58210 to 0.61000, saving model to Best_weight-09-0.61.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.0404 - accuracy: 0.6305 - val_loss: 1.1195 - val_accuracy: 0.6100\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0045 - accuracy: 0.6420\n",
            "Epoch 00010: val_accuracy improved from 0.61000 to 0.61620, saving model to Best_weight-10-0.62.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.0045 - accuracy: 0.6420 - val_loss: 1.1945 - val_accuracy: 0.6162\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.9713 - accuracy: 0.6590\n",
            "Epoch 00011: val_accuracy improved from 0.61620 to 0.62350, saving model to Best_weight-11-0.62.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.9713 - accuracy: 0.6590 - val_loss: 1.1637 - val_accuracy: 0.6235\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.9339 - accuracy: 0.6688\n",
            "Epoch 00012: val_accuracy did not improve from 0.62350\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.9339 - accuracy: 0.6688 - val_loss: 1.1979 - val_accuracy: 0.6054\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.6792\n",
            "Epoch 00013: val_accuracy improved from 0.62350 to 0.64970, saving model to Best_weight-13-0.65.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.9090 - accuracy: 0.6792 - val_loss: 1.0705 - val_accuracy: 0.6497\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8868 - accuracy: 0.6886\n",
            "Epoch 00014: val_accuracy did not improve from 0.64970\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.8868 - accuracy: 0.6886 - val_loss: 1.4066 - val_accuracy: 0.5906\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8651 - accuracy: 0.6957\n",
            "Epoch 00015: val_accuracy improved from 0.64970 to 0.66200, saving model to Best_weight-15-0.66.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.8651 - accuracy: 0.6957 - val_loss: 1.0177 - val_accuracy: 0.6620\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8388 - accuracy: 0.7059\n",
            "Epoch 00016: val_accuracy improved from 0.66200 to 0.69850, saving model to Best_weight-16-0.70.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.8388 - accuracy: 0.7059 - val_loss: 0.8595 - val_accuracy: 0.6985\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8126 - accuracy: 0.7142\n",
            "Epoch 00017: val_accuracy did not improve from 0.69850\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.8126 - accuracy: 0.7142 - val_loss: 1.0189 - val_accuracy: 0.6640\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8099 - accuracy: 0.7163\n",
            "Epoch 00018: val_accuracy improved from 0.69850 to 0.71000, saving model to Best_weight-18-0.71.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.8099 - accuracy: 0.7163 - val_loss: 0.8834 - val_accuracy: 0.7100\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.7274\n",
            "Epoch 00019: val_accuracy did not improve from 0.71000\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.7824 - accuracy: 0.7274 - val_loss: 0.9818 - val_accuracy: 0.6835\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7629 - accuracy: 0.7314\n",
            "Epoch 00020: val_accuracy improved from 0.71000 to 0.71690, saving model to Best_weight-20-0.72.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.7629 - accuracy: 0.7314 - val_loss: 0.8437 - val_accuracy: 0.7169\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7506 - accuracy: 0.7392\n",
            "Epoch 00021: val_accuracy did not improve from 0.71690\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.7506 - accuracy: 0.7392 - val_loss: 0.9850 - val_accuracy: 0.6767\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.7398\n",
            "Epoch 00022: val_accuracy did not improve from 0.71690\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.7391 - accuracy: 0.7398 - val_loss: 1.2700 - val_accuracy: 0.6202\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7247 - accuracy: 0.7456\n",
            "Epoch 00023: val_accuracy did not improve from 0.71690\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.7247 - accuracy: 0.7456 - val_loss: 0.9389 - val_accuracy: 0.6874\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.7532\n",
            "Epoch 00024: val_accuracy did not improve from 0.71690\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.7141 - accuracy: 0.7532 - val_loss: 1.0034 - val_accuracy: 0.6915\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.7548\n",
            "Epoch 00025: val_accuracy did not improve from 0.71690\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6975 - accuracy: 0.7548 - val_loss: 1.1864 - val_accuracy: 0.6552\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.7608\n",
            "Epoch 00026: val_accuracy improved from 0.71690 to 0.73890, saving model to Best_weight-26-0.74.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6858 - accuracy: 0.7608 - val_loss: 0.7886 - val_accuracy: 0.7389\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.7654\n",
            "Epoch 00027: val_accuracy did not improve from 0.73890\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6712 - accuracy: 0.7654 - val_loss: 0.8027 - val_accuracy: 0.7295\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.7670\n",
            "Epoch 00028: val_accuracy did not improve from 0.73890\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6663 - accuracy: 0.7670 - val_loss: 0.9862 - val_accuracy: 0.6866\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.7741\n",
            "Epoch 00029: val_accuracy improved from 0.73890 to 0.75300, saving model to Best_weight-29-0.75.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6497 - accuracy: 0.7741 - val_loss: 0.7320 - val_accuracy: 0.7530\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.7755\n",
            "Epoch 00030: val_accuracy did not improve from 0.75300\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.6435 - accuracy: 0.7755 - val_loss: 1.2393 - val_accuracy: 0.6503\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.7817\n",
            "Epoch 00031: val_accuracy improved from 0.75300 to 0.76890, saving model to Best_weight-31-0.77.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6314 - accuracy: 0.7817 - val_loss: 0.6850 - val_accuracy: 0.7689\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.7811\n",
            "Epoch 00032: val_accuracy did not improve from 0.76890\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6306 - accuracy: 0.7811 - val_loss: 0.7836 - val_accuracy: 0.7407\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.7821\n",
            "Epoch 00033: val_accuracy did not improve from 0.76890\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6222 - accuracy: 0.7821 - val_loss: 0.8162 - val_accuracy: 0.7471\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.7893\n",
            "Epoch 00034: val_accuracy did not improve from 0.76890\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6080 - accuracy: 0.7893 - val_loss: 0.6948 - val_accuracy: 0.7687\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7902\n",
            "Epoch 00035: val_accuracy improved from 0.76890 to 0.77730, saving model to Best_weight-35-0.78.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.6000 - accuracy: 0.7902 - val_loss: 0.6477 - val_accuracy: 0.7773\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.7920\n",
            "Epoch 00036: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5975 - accuracy: 0.7920 - val_loss: 0.9849 - val_accuracy: 0.7128\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.7947\n",
            "Epoch 00037: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5883 - accuracy: 0.7947 - val_loss: 0.7902 - val_accuracy: 0.7477\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.7994\n",
            "Epoch 00038: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5785 - accuracy: 0.7994 - val_loss: 1.0327 - val_accuracy: 0.6908\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.8038\n",
            "Epoch 00039: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5680 - accuracy: 0.8038 - val_loss: 0.7483 - val_accuracy: 0.7544\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.8050\n",
            "Epoch 00040: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.5645 - accuracy: 0.8050 - val_loss: 0.7561 - val_accuracy: 0.7523\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.8032\n",
            "Epoch 00041: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5630 - accuracy: 0.8032 - val_loss: 0.8516 - val_accuracy: 0.7251\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8062\n",
            "Epoch 00042: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.5542 - accuracy: 0.8062 - val_loss: 0.7849 - val_accuracy: 0.7421\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8077\n",
            "Epoch 00043: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5495 - accuracy: 0.8077 - val_loss: 0.9195 - val_accuracy: 0.7233\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.8113\n",
            "Epoch 00044: val_accuracy did not improve from 0.77730\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.5412 - accuracy: 0.8113 - val_loss: 0.8203 - val_accuracy: 0.7422\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.8111\n",
            "Epoch 00045: val_accuracy improved from 0.77730 to 0.78030, saving model to Best_weight-45-0.78.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5420 - accuracy: 0.8111 - val_loss: 0.6703 - val_accuracy: 0.7803\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.8140\n",
            "Epoch 00046: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5345 - accuracy: 0.8140 - val_loss: 0.7035 - val_accuracy: 0.7733\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.8162\n",
            "Epoch 00047: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5274 - accuracy: 0.8162 - val_loss: 0.8364 - val_accuracy: 0.7347\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.8177\n",
            "Epoch 00048: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.5187 - accuracy: 0.8177 - val_loss: 0.8103 - val_accuracy: 0.7469\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.8186\n",
            "Epoch 00049: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 222s 2s/step - loss: 0.5174 - accuracy: 0.8186 - val_loss: 0.7544 - val_accuracy: 0.7609\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8186\n",
            "Epoch 00050: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.5191 - accuracy: 0.8186 - val_loss: 0.7388 - val_accuracy: 0.7625\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.8243\n",
            "Epoch 00051: val_accuracy did not improve from 0.78030\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.5057 - accuracy: 0.8243 - val_loss: 0.6908 - val_accuracy: 0.7769\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4953 - accuracy: 0.8257\n",
            "Epoch 00052: val_accuracy improved from 0.78030 to 0.79640, saving model to Best_weight-52-0.80.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4953 - accuracy: 0.8257 - val_loss: 0.6365 - val_accuracy: 0.7964\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.8280\n",
            "Epoch 00053: val_accuracy improved from 0.79640 to 0.81660, saving model to Best_weight-53-0.82.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4959 - accuracy: 0.8280 - val_loss: 0.5597 - val_accuracy: 0.8166\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4914 - accuracy: 0.8301\n",
            "Epoch 00054: val_accuracy did not improve from 0.81660\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4914 - accuracy: 0.8301 - val_loss: 0.7866 - val_accuracy: 0.7563\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4818 - accuracy: 0.8340\n",
            "Epoch 00055: val_accuracy did not improve from 0.81660\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4818 - accuracy: 0.8340 - val_loss: 0.7368 - val_accuracy: 0.7654\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.8318\n",
            "Epoch 00056: val_accuracy did not improve from 0.81660\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4806 - accuracy: 0.8318 - val_loss: 0.8964 - val_accuracy: 0.7426\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8312\n",
            "Epoch 00057: val_accuracy improved from 0.81660 to 0.82010, saving model to Best_weight-57-0.82.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4843 - accuracy: 0.8312 - val_loss: 0.5231 - val_accuracy: 0.8201\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.8346\n",
            "Epoch 00058: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 222s 2s/step - loss: 0.4742 - accuracy: 0.8346 - val_loss: 0.7140 - val_accuracy: 0.7780\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.8367\n",
            "Epoch 00059: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4704 - accuracy: 0.8367 - val_loss: 0.5739 - val_accuracy: 0.8111\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.8384\n",
            "Epoch 00060: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4659 - accuracy: 0.8384 - val_loss: 0.8237 - val_accuracy: 0.7577\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.8414\n",
            "Epoch 00061: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4597 - accuracy: 0.8414 - val_loss: 0.6071 - val_accuracy: 0.8023\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8398\n",
            "Epoch 00062: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4621 - accuracy: 0.8398 - val_loss: 0.6827 - val_accuracy: 0.7781\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.8411\n",
            "Epoch 00063: val_accuracy did not improve from 0.82010\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4574 - accuracy: 0.8411 - val_loss: 0.5685 - val_accuracy: 0.8103\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.8428\n",
            "Epoch 00064: val_accuracy improved from 0.82010 to 0.82960, saving model to Best_weight-64-0.83.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4522 - accuracy: 0.8428 - val_loss: 0.5166 - val_accuracy: 0.8296\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.8443\n",
            "Epoch 00065: val_accuracy did not improve from 0.82960\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4501 - accuracy: 0.8443 - val_loss: 0.7323 - val_accuracy: 0.7798\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8427\n",
            "Epoch 00066: val_accuracy did not improve from 0.82960\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4479 - accuracy: 0.8427 - val_loss: 0.6955 - val_accuracy: 0.7862\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.8434\n",
            "Epoch 00067: val_accuracy did not improve from 0.82960\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4476 - accuracy: 0.8434 - val_loss: 0.5229 - val_accuracy: 0.8268\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8462\n",
            "Epoch 00068: val_accuracy did not improve from 0.82960\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.4409 - accuracy: 0.8462 - val_loss: 0.6083 - val_accuracy: 0.8113\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.8479\n",
            "Epoch 00069: val_accuracy did not improve from 0.82960\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4398 - accuracy: 0.8479 - val_loss: 0.5678 - val_accuracy: 0.8198\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.8509\n",
            "Epoch 00070: val_accuracy improved from 0.82960 to 0.83170, saving model to Best_weight-70-0.83.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4328 - accuracy: 0.8509 - val_loss: 0.5184 - val_accuracy: 0.8317\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8519\n",
            "Epoch 00071: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4250 - accuracy: 0.8519 - val_loss: 0.6561 - val_accuracy: 0.7936\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8490\n",
            "Epoch 00072: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4273 - accuracy: 0.8490 - val_loss: 0.6437 - val_accuracy: 0.7963\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8526\n",
            "Epoch 00073: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4233 - accuracy: 0.8526 - val_loss: 0.6158 - val_accuracy: 0.7967\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.8533\n",
            "Epoch 00074: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 222s 2s/step - loss: 0.4243 - accuracy: 0.8533 - val_loss: 0.9747 - val_accuracy: 0.7220\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4181 - accuracy: 0.8535\n",
            "Epoch 00075: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4181 - accuracy: 0.8535 - val_loss: 0.6490 - val_accuracy: 0.7980\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8556\n",
            "Epoch 00076: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4150 - accuracy: 0.8556 - val_loss: 0.5774 - val_accuracy: 0.8150\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4161 - accuracy: 0.8539\n",
            "Epoch 00077: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4161 - accuracy: 0.8539 - val_loss: 0.5380 - val_accuracy: 0.8249\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8562\n",
            "Epoch 00078: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4129 - accuracy: 0.8562 - val_loss: 0.5820 - val_accuracy: 0.8051\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.8595\n",
            "Epoch 00079: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4052 - accuracy: 0.8595 - val_loss: 0.5460 - val_accuracy: 0.8253\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4069 - accuracy: 0.8578\n",
            "Epoch 00080: val_accuracy did not improve from 0.83170\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4069 - accuracy: 0.8578 - val_loss: 0.7670 - val_accuracy: 0.7715\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8599\n",
            "Epoch 00081: val_accuracy improved from 0.83170 to 0.84110, saving model to Best_weight-81-0.84.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.4051 - accuracy: 0.8599 - val_loss: 0.4844 - val_accuracy: 0.8411\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8623\n",
            "Epoch 00082: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3951 - accuracy: 0.8623 - val_loss: 0.8393 - val_accuracy: 0.7607\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8642\n",
            "Epoch 00083: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3970 - accuracy: 0.8642 - val_loss: 0.4961 - val_accuracy: 0.8388\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8630\n",
            "Epoch 00084: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 215s 2s/step - loss: 0.3970 - accuracy: 0.8630 - val_loss: 0.6185 - val_accuracy: 0.8039\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.8635\n",
            "Epoch 00085: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3933 - accuracy: 0.8635 - val_loss: 0.4858 - val_accuracy: 0.8382\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8644\n",
            "Epoch 00086: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3894 - accuracy: 0.8644 - val_loss: 0.5797 - val_accuracy: 0.8245\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.8633\n",
            "Epoch 00087: val_accuracy did not improve from 0.84110\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3923 - accuracy: 0.8633 - val_loss: 0.4812 - val_accuracy: 0.8410\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8650\n",
            "Epoch 00088: val_accuracy improved from 0.84110 to 0.84220, saving model to Best_weight-88-0.84.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3883 - accuracy: 0.8650 - val_loss: 0.4843 - val_accuracy: 0.8422\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8645\n",
            "Epoch 00089: val_accuracy did not improve from 0.84220\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3880 - accuracy: 0.8645 - val_loss: 0.9350 - val_accuracy: 0.7535\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8677\n",
            "Epoch 00090: val_accuracy did not improve from 0.84220\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3827 - accuracy: 0.8677 - val_loss: 0.4870 - val_accuracy: 0.8422\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3767 - accuracy: 0.8693\n",
            "Epoch 00091: val_accuracy did not improve from 0.84220\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3767 - accuracy: 0.8693 - val_loss: 0.5435 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8685\n",
            "Epoch 00092: val_accuracy did not improve from 0.84220\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3759 - accuracy: 0.8685 - val_loss: 0.6893 - val_accuracy: 0.7900\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8690\n",
            "Epoch 00093: val_accuracy improved from 0.84220 to 0.84550, saving model to Best_weight-93-0.85.hdf5\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3731 - accuracy: 0.8690 - val_loss: 0.4797 - val_accuracy: 0.8455\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8696\n",
            "Epoch 00094: val_accuracy did not improve from 0.84550\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3742 - accuracy: 0.8696 - val_loss: 0.5471 - val_accuracy: 0.8303\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.8671\n",
            "Epoch 00095: val_accuracy did not improve from 0.84550\n",
            "98/98 [==============================] - 216s 2s/step - loss: 0.3783 - accuracy: 0.8671 - val_loss: 0.5419 - val_accuracy: 0.8251\n",
            "Epoch 96/100\n",
            "73/98 [=====================>........] - ETA: 51s - loss: 0.3694 - accuracy: 0.8706"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIW2VB0jaIyu"
      },
      "source": [
        "model_2.save(\"model_2_100.h5\")\n",
        "#https://github.com/aayushs879/Densenet-on-CIFAR-10/blob/master/DNST.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zop5TdA8dajp"
      },
      "source": [
        "model_2 = tf.keras.models.load_model(\"/content/model_2_50.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9yKc_mgtpn"
      },
      "source": [
        "import os\n",
        "checkpoint_path = \"training_2/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 monitor = 'val_loss',\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa6lXhGUgNzg",
        "outputId": "e11c9730-00b8-4b03-9b35-7f550ab015bb"
      },
      "source": [
        "model_2.fit(train_generator.flow(X_train,y_train,batch_size= batch_size),\n",
        "            epochs= 50,verbose= 1,validation_data=validation_generator.flow(X_test,y_test,batch_size = 256),\n",
        "            callbacks = [cp_callback,tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 328s 2s/step - loss: 0.4901 - accuracy: 0.8307 - val_loss: 0.5754 - val_accuracy: 0.8110\n",
            "\n",
            "Epoch 00001: saving model to training_2/cp.ckpt\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 325s 2s/step - loss: 0.4850 - accuracy: 0.8314 - val_loss: 0.5759 - val_accuracy: 0.8117\n",
            "\n",
            "Epoch 00002: saving model to training_2/cp.ckpt\n",
            "Epoch 3/50\n",
            " 77/196 [==========>...................] - ETA: 3:06 - loss: 0.4766 - accuracy: 0.8344"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_8gTuuigwRP"
      },
      "source": [
        "def preprocess_image(x_train,x_test):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    x_train --> training dataframe - type pandas -core-DataFrame\n",
        "\n",
        "    x_test --> test dataframe - type pandas -core-DataFrame\n",
        "\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  #preprocess the image scaling\n",
        "  #Reference : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "  train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range = 20, horizontal_flip = True, \n",
        "                                                                    width_shift_range = 0.1, height_shift_range = 0.1,\n",
        "                                                                    zoom_range = 0.3, shear_range = 15)\n",
        "  \n",
        "\n",
        "  train_generator.fit(x_train)\n",
        "\n",
        " \n",
        "  validation_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  validation_generator.fit(x_test)\n",
        "\n",
        "  return train_generator,validation_generator"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}